{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Embedded Audio Signal Processing This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ), INRIA , and GRAME-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the resources of the CITI/INRIA Emeraude Team (Embedded Programmable Audio Systems) in this domain. In this course, students will learn about: Low-level embedded systems for real-time audio signal processing Digital audio system architecture Audio codec configuration IC communication protocols Audio signal processing Audio sound synthesis and effects design The Faust programming language Instructors Romain Michon (INRIA) Tanguy Risset (INSA Lyon) Yann Orlarey (GRAME-CNCM) Organization and ECTS The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs Course Overview Lecture 1: Course Introduction and Programming Environment Setup -- 15/11/2022 14h00-16h00 Lecture 2: Audio Signal Processing Fundamentals -- 16/11/2022 14h00-16h00 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 16/11/2022 16h00-18h00 Lecture 4: Hardware Control and Audio Codec Configuration -- 22/11/2022 14h00-16h00 Lecture 5: Audio Processing Basics I -- 23/11/2022 14h00-16h00 Lecture 6: Audio Processing Basics II -- 23/11/2022 16h00-18h00 Lecture 7: TBD -- 29/11/2022 14h00-16h00 Lecture 8: Faust Tutorial -- 30/11/2022 14h00-16h00 Lecture 9: Embedded System Peripherals -- 30/11/2022 16h00-18h00 Lecture 10: Embedded OS, FreeRTOS, Embedded Linux Devices -- 06/12/2022 14h00-16h00 Lecture 11: Faust on the Teensy and Advanced Control -- 07/12/2022 14h00-16h00 Sessions 12-16: Mini-project 07/12/2022 16h00-18h00 12/12/2022 14h00-16h00 12/12/2022 16h00-18h00 14/12/2022 14h00-16h00 14/12/2022 16h00-18h00","title":"Syllabus"},{"location":"#embedded-audio-signal-processing","text":"This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ), INRIA , and GRAME-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the resources of the CITI/INRIA Emeraude Team (Embedded Programmable Audio Systems) in this domain. In this course, students will learn about: Low-level embedded systems for real-time audio signal processing Digital audio system architecture Audio codec configuration IC communication protocols Audio signal processing Audio sound synthesis and effects design The Faust programming language","title":"Embedded Audio Signal Processing"},{"location":"#instructors","text":"Romain Michon (INRIA) Tanguy Risset (INSA Lyon) Yann Orlarey (GRAME-CNCM)","title":"Instructors"},{"location":"#organization-and-ects","text":"The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs","title":"Organization and ECTS"},{"location":"#course-overview","text":"Lecture 1: Course Introduction and Programming Environment Setup -- 15/11/2022 14h00-16h00 Lecture 2: Audio Signal Processing Fundamentals -- 16/11/2022 14h00-16h00 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 16/11/2022 16h00-18h00 Lecture 4: Hardware Control and Audio Codec Configuration -- 22/11/2022 14h00-16h00 Lecture 5: Audio Processing Basics I -- 23/11/2022 14h00-16h00 Lecture 6: Audio Processing Basics II -- 23/11/2022 16h00-18h00 Lecture 7: TBD -- 29/11/2022 14h00-16h00 Lecture 8: Faust Tutorial -- 30/11/2022 14h00-16h00 Lecture 9: Embedded System Peripherals -- 30/11/2022 16h00-18h00 Lecture 10: Embedded OS, FreeRTOS, Embedded Linux Devices -- 06/12/2022 14h00-16h00 Lecture 11: Faust on the Teensy and Advanced Control -- 07/12/2022 14h00-16h00 Sessions 12-16: Mini-project 07/12/2022 16h00-18h00 12/12/2022 14h00-16h00 12/12/2022 16h00-18h00 14/12/2022 14h00-16h00 14/12/2022 16h00-18h00","title":"Course Overview"},{"location":"lectures/lecture1/","text":"Lecture 1: Course Introduction and Programming Environment Setup This lecture is devoted to the software suite install so that everybody can follow the other lectures from Insa or from his home if it needs to be done in distant work. Course outline All Lecture (2h on a computer) are labs on the LyraT board Part 1 : Board introduction and Audio Signal Processing Basics Lecture 1 (2h): Course Introduction, programming env setup Lecture 2: Basic signal processing, Lecture 3: Audio systems architecture, audio callback Lecture 4: Hardware Audio codec configuration Lecture 5 & 6: Audio signal processing basics synthesis Lecture 7: Faust Language tutorial https://faust.grame.fr/ Part 2: Embedded Audio System Architecture Lecture 8: RTone comference on embedded systems in industry https://rtone.fr/ Lecture 9: TBD Lecture 10: TDB Part 3: LyraT programming Lecture 11-14: mini project Lecture 15-16: demonstrations Introduction to AUD2020 and Teensy Teensy 4.0 from [PRJC](https://www.pjrc.com/store/teensy40.html), and the associated audio adaptor board The development in AUD are performed on teensy which is developped by PJRC. It is a microcontroller that offers many I/O pins and a USB interface. It is programmed using specialization of the arduino programming environnement ( teensyduino ). Teensy is a brand of microcontroller development boards designed by the co-owner of PJRC, Paul Stoffregen . The first Teensy 2.0, Teensy++ 2.0 (and discontinued predecessors) use an 8-bit AVR microcontrollers. Teensy 3.0 (and up) have instead Freescale microcontrollers, running ARM Cortex-M CPUs. The technical characteritics of all Teensy can be compared here In AUD, we use Teensy 4.0 which contain an ARM Cortex-M7 at 600 MHz with a Floating point unit, hence it can handle non trivial audio treatment. Teensy 4.0 from [PRJC](https://www.pjrc.com/store/teensy40.html), and the associated audio adaptor board Teensy 4.0 and Audio shield (from PJRC website). Teensy 4.0 uses many powerful CPU features useful for true real-time microcontroller platform. The CPU is an ARM Cortex-M7 dual-issue superscaler clocked at at 600 MHz. CPU performance is many times faster than typical 32 bit microcontrollers. The Floating Point Unit performs 32 bit float and 64 bit double precision math in hardware. DSP extension instructions accelerate signal processing, filters and Fourier transform. The Audio library automatically makes uses of these DSP instructions. Teensy performance from Core Benchmarks This pinout reference card comes with Teensy 4.0 ( do not loose it! ). The pins are not 5V tolerant. Do not drive any digital pin higher than 3.3V. Teensy 4.0 pin map (from PJRC webite) Teensy 4.0 has a total of 40 input/output signal pins. 24 are easily accessible when used with a solderless breadboard. The available pins include general purpose IO (GPIO, digital or analog, i.e. ADC), as well as integrated serial protocols (I2C, I2S, CAN, SPI and UART protocols) that are used to connect to other devices. In AUD, we use the audio adaptor board provided by PJRC that integrates a low power stereo codex (NXP Semiconductors SGTL5000 codec) and a SD card reader. Teensy audio adaptor (rev. D) from [PRJC](https://www.pjrc.com/store/teensy40.html), The audio codec connects to Teensy using 7 signals (Yellow signal in pin map above) which are used by two protocol: I2C and I2S. This is a traditionnal configuration for audio codec: the I2C (or I\u00b2C: Inter-Integrated Circuit) protocol is used to configure the codec (sample rate, input and output pins etc.) and the I2S (or I\u00b2S: Inter-IC Sound) is used to transfer samples bit by bit in both direction (i.e. from and to the teensy). The I2C pins SDA and SCL are used to control the chip and adjust parameters. Audio data uses I2S signals, TX (to headphones and/or line out) and RX (from line in or mic), and 3 clocks, LRCLK (44.1 kHz), BCLK (1.41 MHz) and MCLK (11.29 MHz). All 3 clocks are created by Teensy which means that the SGTL5000 operates in \"slave mode\". The schematics of the audio shield board, rev. D, can bee seen here and the sch\u00e9matic of the Teensy 4.0 can be seen at the end of the page here . Of course, as they are both made by PJRC, they are designed to be compatible. We (the teachers!) have soldered the connectors so that the audio shiel can be easily connected to the tennsy. The USB connector of the Teensy can support many serial communication from the host computer to the Teensy: (JTAG for flashing/programming, Serial UART, midi, mouse etc. see Tools -> USB Type menu in arduino IDE). In AUD it is mainly used to program the device (i.e. download binary code into flash memory) and textual communication between the host and the Teensy using UART communicatino protocol (Serial). In linux machine, the serial port will appear as /dev/ttyACM0 Teensy 4.0 has 2 Mbyte of flash memory intended for storing your code. The flash memory can also store read-only variables and arrays.1024K of memory is available for variables and data. Half of this memory (RAM1) is accessed as tightly coupled memory for maximum performance. The other half (RAM2) is optimized for access by DMA. Normally large arrays & data buffers are placed in RAM2, to save the ultra-fast RAM1 for normal variables. The memory map is the following: Teensy 4.0 pin map (from PJRC webite) When the compiler builds your program, all global variables, static variables, and compiled code is assigned to dedicated locations in memory. This is called static allocation, because the memory addresses are fixed. By default, allocation tries to use the ultra-fast DTCM & ITCM memory. The following keywords allow control over where the compiler will place your variables and code within the memory. {TODO: shorten that:} DMAMEM - Variables defined with DMAMEM are placed at the beginning of RAM2. Normally buffers and large arrays are placed here. These variables can not be initialized, your program must write their initial values, if needed. PROGMEM & F() - Variables defined with PROGMEM, and strings surrounded by F() are placed only in the flash memory. They can be accessed normally, special functions normally used on 8 bit boards are not required to read PROGMEM variables. FASTRUN - Functions defined with \"FASTRUN\" are allocated in the beginning of RAM1. A copy is also stored in Flash and copied to RAM1 at startup. These functions are accessed by the Cortex-M7 ITCM bus, for the fastest possible performance. By default, functions without any memory type defined are treated as FASTRUN. A small amount of memory is typically unused, because the ITCM bus must access a memory region which is a multiple of 32K. FLASHMEM - Functions defined with \"FLASHMEM\" executed directly from Flash. If the Cortex-M7 cache is not already holding a copy of the function, a delay results while the Flash memory is read into the M7's cache. FLASHMEM should be used on startup code and other functions where speed is not important. As your program runs, it may use all of the RAM which was not reserved by static allocation. Because the specific memory address for each variable is computed as your program runs, this is called dynamic memory allocation. Local Variables - Local variables, and also return addresses from function calls and the saved state from interrupts are placed on a stack which starts from the top of RAM1 and grown downward. The amount of space for local variable is the portion of RAM1 not used by FASTRUN code and the initialized and zeroed variables. Heap - Memory allocated by C++ \"new\" and C malloc(), and Arduino String variables are placed in RAM2, starting immediately after the DMAMEM variables. Teensy developpement framework: teensyduino Arduino's IDE software with the Teensyduino add-on is the primary programming environment for Teensy. Other environment can be used: Visual Micro , PlatformIO or traditionnal command line Makefile (type make in directory $(arduino)/{Arduino}/hardware/teensy/avr/cores/teensy4/ ). TEENSYDUINO PROGRAMMING Installing teensyduino on your computer Getting Started on TC Machines Flashing the LED. Known Problems","title":"Lecture 1: Course Introduction and Programming Environment Setup"},{"location":"lectures/lecture1/#lecture-1-course-introduction-and-programming-environment-setup","text":"This lecture is devoted to the software suite install so that everybody can follow the other lectures from Insa or from his home if it needs to be done in distant work.","title":"Lecture 1: Course Introduction and Programming Environment Setup"},{"location":"lectures/lecture1/#course-outline","text":"All Lecture (2h on a computer) are labs on the LyraT board","title":"Course outline"},{"location":"lectures/lecture1/#part-1-board-introduction-and-audio-signal-processing-basics","text":"Lecture 1 (2h): Course Introduction, programming env setup Lecture 2: Basic signal processing, Lecture 3: Audio systems architecture, audio callback Lecture 4: Hardware Audio codec configuration Lecture 5 & 6: Audio signal processing basics synthesis Lecture 7: Faust Language tutorial https://faust.grame.fr/","title":"Part 1 : Board introduction and Audio Signal Processing Basics "},{"location":"lectures/lecture1/#part-2-embedded-audio-system-architecture","text":"Lecture 8: RTone comference on embedded systems in industry https://rtone.fr/ Lecture 9: TBD Lecture 10: TDB","title":"Part 2: Embedded Audio System Architecture"},{"location":"lectures/lecture1/#part-3-lyrat-programming","text":"Lecture 11-14: mini project Lecture 15-16: demonstrations","title":"Part 3: LyraT programming"},{"location":"lectures/lecture1/#introduction-to-aud2020-and-teensy","text":"Teensy 4.0 from [PRJC](https://www.pjrc.com/store/teensy40.html), and the associated audio adaptor board The development in AUD are performed on teensy which is developped by PJRC. It is a microcontroller that offers many I/O pins and a USB interface. It is programmed using specialization of the arduino programming environnement ( teensyduino ). Teensy is a brand of microcontroller development boards designed by the co-owner of PJRC, Paul Stoffregen . The first Teensy 2.0, Teensy++ 2.0 (and discontinued predecessors) use an 8-bit AVR microcontrollers. Teensy 3.0 (and up) have instead Freescale microcontrollers, running ARM Cortex-M CPUs. The technical characteritics of all Teensy can be compared here In AUD, we use Teensy 4.0 which contain an ARM Cortex-M7 at 600 MHz with a Floating point unit, hence it can handle non trivial audio treatment. Teensy 4.0 from [PRJC](https://www.pjrc.com/store/teensy40.html), and the associated audio adaptor board","title":"Introduction to AUD2020 and Teensy"},{"location":"lectures/lecture1/#teensy-40-and-audio-shield-from-pjrc-website","text":"Teensy 4.0 uses many powerful CPU features useful for true real-time microcontroller platform. The CPU is an ARM Cortex-M7 dual-issue superscaler clocked at at 600 MHz. CPU performance is many times faster than typical 32 bit microcontrollers. The Floating Point Unit performs 32 bit float and 64 bit double precision math in hardware. DSP extension instructions accelerate signal processing, filters and Fourier transform. The Audio library automatically makes uses of these DSP instructions. Teensy performance from Core Benchmarks This pinout reference card comes with Teensy 4.0 ( do not loose it! ). The pins are not 5V tolerant. Do not drive any digital pin higher than 3.3V. Teensy 4.0 pin map (from PJRC webite) Teensy 4.0 has a total of 40 input/output signal pins. 24 are easily accessible when used with a solderless breadboard. The available pins include general purpose IO (GPIO, digital or analog, i.e. ADC), as well as integrated serial protocols (I2C, I2S, CAN, SPI and UART protocols) that are used to connect to other devices. In AUD, we use the audio adaptor board provided by PJRC that integrates a low power stereo codex (NXP Semiconductors SGTL5000 codec) and a SD card reader. Teensy audio adaptor (rev. D) from [PRJC](https://www.pjrc.com/store/teensy40.html), The audio codec connects to Teensy using 7 signals (Yellow signal in pin map above) which are used by two protocol: I2C and I2S. This is a traditionnal configuration for audio codec: the I2C (or I\u00b2C: Inter-Integrated Circuit) protocol is used to configure the codec (sample rate, input and output pins etc.) and the I2S (or I\u00b2S: Inter-IC Sound) is used to transfer samples bit by bit in both direction (i.e. from and to the teensy). The I2C pins SDA and SCL are used to control the chip and adjust parameters. Audio data uses I2S signals, TX (to headphones and/or line out) and RX (from line in or mic), and 3 clocks, LRCLK (44.1 kHz), BCLK (1.41 MHz) and MCLK (11.29 MHz). All 3 clocks are created by Teensy which means that the SGTL5000 operates in \"slave mode\". The schematics of the audio shield board, rev. D, can bee seen here and the sch\u00e9matic of the Teensy 4.0 can be seen at the end of the page here . Of course, as they are both made by PJRC, they are designed to be compatible. We (the teachers!) have soldered the connectors so that the audio shiel can be easily connected to the tennsy. The USB connector of the Teensy can support many serial communication from the host computer to the Teensy: (JTAG for flashing/programming, Serial UART, midi, mouse etc. see Tools -> USB Type menu in arduino IDE). In AUD it is mainly used to program the device (i.e. download binary code into flash memory) and textual communication between the host and the Teensy using UART communicatino protocol (Serial). In linux machine, the serial port will appear as /dev/ttyACM0 Teensy 4.0 has 2 Mbyte of flash memory intended for storing your code. The flash memory can also store read-only variables and arrays.1024K of memory is available for variables and data. Half of this memory (RAM1) is accessed as tightly coupled memory for maximum performance. The other half (RAM2) is optimized for access by DMA. Normally large arrays & data buffers are placed in RAM2, to save the ultra-fast RAM1 for normal variables. The memory map is the following: Teensy 4.0 pin map (from PJRC webite) When the compiler builds your program, all global variables, static variables, and compiled code is assigned to dedicated locations in memory. This is called static allocation, because the memory addresses are fixed. By default, allocation tries to use the ultra-fast DTCM & ITCM memory. The following keywords allow control over where the compiler will place your variables and code within the memory. {TODO: shorten that:} DMAMEM - Variables defined with DMAMEM are placed at the beginning of RAM2. Normally buffers and large arrays are placed here. These variables can not be initialized, your program must write their initial values, if needed. PROGMEM & F() - Variables defined with PROGMEM, and strings surrounded by F() are placed only in the flash memory. They can be accessed normally, special functions normally used on 8 bit boards are not required to read PROGMEM variables. FASTRUN - Functions defined with \"FASTRUN\" are allocated in the beginning of RAM1. A copy is also stored in Flash and copied to RAM1 at startup. These functions are accessed by the Cortex-M7 ITCM bus, for the fastest possible performance. By default, functions without any memory type defined are treated as FASTRUN. A small amount of memory is typically unused, because the ITCM bus must access a memory region which is a multiple of 32K. FLASHMEM - Functions defined with \"FLASHMEM\" executed directly from Flash. If the Cortex-M7 cache is not already holding a copy of the function, a delay results while the Flash memory is read into the M7's cache. FLASHMEM should be used on startup code and other functions where speed is not important. As your program runs, it may use all of the RAM which was not reserved by static allocation. Because the specific memory address for each variable is computed as your program runs, this is called dynamic memory allocation. Local Variables - Local variables, and also return addresses from function calls and the saved state from interrupts are placed on a stack which starts from the top of RAM1 and grown downward. The amount of space for local variable is the portion of RAM1 not used by FASTRUN code and the initialized and zeroed variables. Heap - Memory allocated by C++ \"new\" and C malloc(), and Arduino String variables are placed in RAM2, starting immediately after the DMAMEM variables.","title":"Teensy 4.0 and Audio shield (from PJRC website)."},{"location":"lectures/lecture1/#teensy-developpement-framework-teensyduino","text":"Arduino's IDE software with the Teensyduino add-on is the primary programming environment for Teensy. Other environment can be used: Visual Micro , PlatformIO or traditionnal command line Makefile (type make in directory $(arduino)/{Arduino}/hardware/teensy/avr/cores/teensy4/ ). TEENSYDUINO PROGRAMMING","title":"Teensy  developpement framework: teensyduino"},{"location":"lectures/lecture1/#installing-teensyduino-on-your-computer","text":"","title":"Installing teensyduino on your computer"},{"location":"lectures/lecture1/#getting-started-on-tc-machines","text":"","title":"Getting Started on TC Machines"},{"location":"lectures/lecture1/#flashing-the-led","text":"","title":"Flashing the LED."},{"location":"lectures/lecture1/#known-problems","text":"","title":"Known Problems"},{"location":"lectures/lecture10/","text":"Lecture 10: Embedded OS, freeRTOS This course will present the important notions of embedded operating systems and explain in more details the FreeRtos operating system used on ESP32. Slides It is (temporarily) available through sildes here Exercices Creating tasks Create an IDF project that creates two FreeTtos Tasks: one is generating number each second, send via a FIFO of length 10 these numbers to the other task which is printing them on uart. The two task will have the same priority: 10. One can use the function esp_random for generating random numbers, xTaskCreate to create task and xQueueCreate , xQueueSend , xQueueReceive to communicate between the tasks. Experimenting priority and starving Try to saturate the FIFO (i.e. send faster than receiving by using vTaskDelay . Change the priority of the receiver task, set the priority to 9, does it starve the sender? Wath happens it the receiver does not receive anymore but executes a stupid loop such as this on: for(;;) { cpt++; }","title":"Lecture 10: Embedded OS, freeRTOS"},{"location":"lectures/lecture10/#lecture-10-embedded-os-freertos","text":"This course will present the important notions of embedded operating systems and explain in more details the FreeRtos operating system used on ESP32.","title":"Lecture 10: Embedded OS, freeRTOS"},{"location":"lectures/lecture10/#slides","text":"It is (temporarily) available through sildes here","title":"Slides"},{"location":"lectures/lecture10/#exercices","text":"","title":"Exercices"},{"location":"lectures/lecture10/#creating-tasks","text":"Create an IDF project that creates two FreeTtos Tasks: one is generating number each second, send via a FIFO of length 10 these numbers to the other task which is printing them on uart. The two task will have the same priority: 10. One can use the function esp_random for generating random numbers, xTaskCreate to create task and xQueueCreate , xQueueSend , xQueueReceive to communicate between the tasks.","title":"Creating tasks"},{"location":"lectures/lecture10/#experimenting-priority-and-starving","text":"Try to saturate the FIFO (i.e. send faster than receiving by using vTaskDelay . Change the priority of the receiver task, set the priority to 9, does it starve the sender? Wath happens it the receiver does not receive anymore but executes a stupid loop such as this on: for(;;) { cpt++; }","title":"Experimenting priority and starving"},{"location":"lectures/lecture11/","text":"Lecture 11: Faust on the LyraT and Advanced Control Generating and Using a Faust C++ Object In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMininal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinial.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth LyraT example project demonstrates how FaustSynth.h can be used. First, it is included in AudioDsp.cpp and the following elements are declared in the corresponding header file : MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of AudioDsp.cpp : fDSP = new mydsp(); fDSP->init(fSampleRate); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[2]; for (int channel = 0; channel < 2; ++channel){ outputs[channel] = new float[fBufferSize]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the audioTask , we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: fDSP->compute(fBufferSize,NULL,outputs); // processing buffers for (int channel = 0; channel < 2; ++channel){ for(int i=0; i<fBufferSize; i++){ samples_data_out[i*fNumOutputs+channel] = outputs[channel][i]*MULT_S16; } } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void AudioDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void AudioDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); } Better Control on the LyraT The control of the parameters of an audio DSP object on the LyraT is relatively limited with what we have seen in this class. In this brief section, we propose a few solutions to overcome this problem. Since they are beyond the scope of this class, we wont go into too many details. MIDI MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. Implementing MIDI USB on the LyraT should be relatively straight forward but it hasn't been done yet. OSC Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 Since the LyraT has Wi-Fi built-in, implementing OSC control should also be relatively straight forward. HTTP Another common way on embedded audio systems to control parameters is to implement a simple web server. A webpage can then present the parameters to control in a user interface, etc. Hardware Control A simple way to offer a better control on the LyraT is to add physical sensors/elements to it (e.g., buttons, potentiometers, etc.). However, even though the LyraT hosts an ESP32 microcontroller, it doesn't provide any analog inputs to plug sensors to them. This can be solved by adding a sensor DAC to this device such as a MCP3008 (8 channel 10-bit ADC with SPI interface). Exercises Faust Triangle Oscillator The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the LyraT. Flanger The Faust libraries host a flanger function : pf.flanger_mono Turn your LyraT into a flanger effect processor!","title":"Lecture 11: Faust on the LyraT and Advanced Control"},{"location":"lectures/lecture11/#lecture-11-faust-on-the-lyrat-and-advanced-control","text":"","title":"Lecture 11: Faust on the LyraT and Advanced Control"},{"location":"lectures/lecture11/#generating-and-using-a-faust-c-object","text":"In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMininal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinial.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth LyraT example project demonstrates how FaustSynth.h can be used. First, it is included in AudioDsp.cpp and the following elements are declared in the corresponding header file : MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of AudioDsp.cpp : fDSP = new mydsp(); fDSP->init(fSampleRate); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[2]; for (int channel = 0; channel < 2; ++channel){ outputs[channel] = new float[fBufferSize]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the audioTask , we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: fDSP->compute(fBufferSize,NULL,outputs); // processing buffers for (int channel = 0; channel < 2; ++channel){ for(int i=0; i<fBufferSize; i++){ samples_data_out[i*fNumOutputs+channel] = outputs[channel][i]*MULT_S16; } } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void AudioDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void AudioDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); }","title":"Generating and Using a Faust C++ Object"},{"location":"lectures/lecture11/#better-control-on-the-lyrat","text":"The control of the parameters of an audio DSP object on the LyraT is relatively limited with what we have seen in this class. In this brief section, we propose a few solutions to overcome this problem. Since they are beyond the scope of this class, we wont go into too many details.","title":"Better Control on the LyraT"},{"location":"lectures/lecture11/#midi","text":"MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. Implementing MIDI USB on the LyraT should be relatively straight forward but it hasn't been done yet.","title":"MIDI"},{"location":"lectures/lecture11/#osc","text":"Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 Since the LyraT has Wi-Fi built-in, implementing OSC control should also be relatively straight forward.","title":"OSC"},{"location":"lectures/lecture11/#http","text":"Another common way on embedded audio systems to control parameters is to implement a simple web server. A webpage can then present the parameters to control in a user interface, etc.","title":"HTTP"},{"location":"lectures/lecture11/#hardware-control","text":"A simple way to offer a better control on the LyraT is to add physical sensors/elements to it (e.g., buttons, potentiometers, etc.). However, even though the LyraT hosts an ESP32 microcontroller, it doesn't provide any analog inputs to plug sensors to them. This can be solved by adding a sensor DAC to this device such as a MCP3008 (8 channel 10-bit ADC with SPI interface).","title":"Hardware Control"},{"location":"lectures/lecture11/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture11/#faust-triangle-oscillator","text":"The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the LyraT.","title":"Faust Triangle Oscillator"},{"location":"lectures/lecture11/#flanger","text":"The Faust libraries host a flanger function : pf.flanger_mono Turn your LyraT into a flanger effect processor!","title":"Flanger"},{"location":"lectures/lecture2/","text":"Lecture 2: Audio Signal Processing Fundamentals The goal of this lecture is to provide an overview of the basics of digital audio. Analog Audio Signals Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer The Discovery of Digital Audio Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia ) ADC and DAC In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.). Human Hearing Range and Sampling Rate One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz. Sampling Theorem Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here . Aliasing Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening. Bit Depth, Dynamic Range and Signal-to-Noise Ratio Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here . Range of Audio Samples Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signals to this range to prevent warping and will hence result in clipping if exceeded. First Synthesized Sound on a Digital Computer While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":" 2: Audio Signal Processing Fundamentals "},{"location":"lectures/lecture2/#lecture-2-audio-signal-processing-fundamentals","text":"The goal of this lecture is to provide an overview of the basics of digital audio.","title":"Lecture 2: Audio Signal Processing Fundamentals"},{"location":"lectures/lecture2/#analog-audio-signals","text":"Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer","title":"Analog Audio Signals"},{"location":"lectures/lecture2/#the-discovery-of-digital-audio","text":"Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia )","title":"The Discovery of Digital Audio"},{"location":"lectures/lecture2/#adc-and-dac","text":"In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.).","title":"ADC and DAC"},{"location":"lectures/lecture2/#human-hearing-range-and-sampling-rate","text":"One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz.","title":"Human Hearing Range and Sampling Rate"},{"location":"lectures/lecture2/#sampling-theorem","text":"Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here .","title":"Sampling Theorem"},{"location":"lectures/lecture2/#aliasing","text":"Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening.","title":"Aliasing"},{"location":"lectures/lecture2/#bit-depth-dynamic-range-and-signal-to-noise-ratio","text":"Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here .","title":"Bit Depth, Dynamic Range and Signal-to-Noise Ratio"},{"location":"lectures/lecture2/#range-of-audio-samples","text":"Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signals to this range to prevent warping and will hence result in clipping if exceeded.","title":"Range of Audio Samples"},{"location":"lectures/lecture2/#first-synthesized-sound-on-a-digital-computer","text":"While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":"First Synthesized Sound on a Digital Computer"},{"location":"lectures/lecture3/","text":"Lecture 3: Digital Audio Systems Architectures and Audio Callback By the end of this lecture, you should be able to produce sound with your Teensy and have a basic understanding of the software and hardware architecture of embedded audio systems. Basic Architecture of a Digital Audio System All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system. Architecture of Embedded Audio Systems Such as the Teensy In embedded audio systems, the component implementing the audio ADC and DAC is called an \"Audio Codec.\" This name is slightly ambiguous because it is also used in the context of audio compression (e.g., mp3) to designate a totally different concept. In the case of the Teensy kits that are provided to you as part of this class, the audio codec we use is an SGTL5000. It is mounted on a shield/sister board that has the same form factor as the Teensy. Audio samples are sent and received between the Cortex M7 and the audio codec using the i2s protocol (additional information on how this kind of system works will be provided in Lecture 4 ). As a microcontroller, the Cortex M7 has its own analog inputs which can be used to retrieve sensor datas (e.g., potentiometers, etc.). These analog inputs cannot be used for audio because of their limited precision and sampling rate. We'll briefly show in Lecture TODO how these analog inputs can be used to use sensors to control audio algorithms running on the Teensy. Teensy and Audio Shield Overview Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the Teensy can achieve much lower latencies than regular computers because of their lightness. Hence, the block size of your Teensy can be as small as 8! First Audio Program on the Teensy: crazy-sine The course repository hosts an example containing a program synthesizing a sine wave on the Teensy and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the update method and take the following shape: #define MULT_16 32767 void MyDsp::update(void) { audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { float currentSample = echo.tick(sine.tick())*0.5; currentSample = max(-1,min(1,currentSample)); int16_t val = currentSample*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } The update method is called every time a new audio buffer is needed by the system. A new audio buffer audioBlock containing AUDIO_OUTPUTS channels is first created. For every audio channel, memory is allocated and a full block of samples is computed. Individual samples resulting from computing a sine wave through an echo ( echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively) are stored in currentSample . currentSample is a floating point number whose range is {-1;1}. This is a standard in the world of digital audio, hence, a signal actually ranging between {-1;1} will correspond to the \"loudest\" sound that can be played on a given system. max(-1,min(1,currentSample)); ensures that currentSample doesn't exceed this range. AUDIO_BLOCK_SAMPLES corresponds to the block size (256 samples by default on the Teensy, but this value can potentially be adjusted). The values contained in currentSample (between -1 and 1) must be converted to 16 bits signed integers (to ensure compatibility with the rest of the Teensy audio library). For that, we just have to multiply currentSample by 2^{16}/2 Note that currentSample is multiplied by 0.5 to control the output gain of the system here (we'll see later in this class that echos tend to add energy to the system hence we must limit the gain of the output signal to prevent potential saturation). Once a full block has been computed, it is transmitted to the rest of the system using the transmit function. Once this is done, the memory that was allocated for the audio block is freed using the release function. The update method is called over and over until the Teensy is powered out. C++ Sine Wave Oscillator Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; } Exercises Looping Through a Small Tune: Making a Music Box In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your Teensy. Hint: For that, you'll probably have to replace the myDsp.setFreq(random(50,1000)); line of of crazy-sine.ino by something else. Solution: Posted after class... Basic Additive Synthesis One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Solution: Posted after class... Stereo Echo Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo. Solution: Posted after class...","title":" 3: Digital Audio Systems Architectures and Audio Callback "},{"location":"lectures/lecture3/#lecture-3-digital-audio-systems-architectures-and-audio-callback","text":"By the end of this lecture, you should be able to produce sound with your Teensy and have a basic understanding of the software and hardware architecture of embedded audio systems.","title":"Lecture 3: Digital Audio Systems Architectures and Audio Callback"},{"location":"lectures/lecture3/#basic-architecture-of-a-digital-audio-system","text":"All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system.","title":"Basic Architecture of a Digital Audio System"},{"location":"lectures/lecture3/#architecture-of-embedded-audio-systems-such-as-the-teensy","text":"In embedded audio systems, the component implementing the audio ADC and DAC is called an \"Audio Codec.\" This name is slightly ambiguous because it is also used in the context of audio compression (e.g., mp3) to designate a totally different concept. In the case of the Teensy kits that are provided to you as part of this class, the audio codec we use is an SGTL5000. It is mounted on a shield/sister board that has the same form factor as the Teensy. Audio samples are sent and received between the Cortex M7 and the audio codec using the i2s protocol (additional information on how this kind of system works will be provided in Lecture 4 ). As a microcontroller, the Cortex M7 has its own analog inputs which can be used to retrieve sensor datas (e.g., potentiometers, etc.). These analog inputs cannot be used for audio because of their limited precision and sampling rate. We'll briefly show in Lecture TODO how these analog inputs can be used to use sensors to control audio algorithms running on the Teensy. Teensy and Audio Shield Overview","title":"Architecture of Embedded Audio Systems Such as the Teensy"},{"location":"lectures/lecture3/#concept-of-audio-blocks-buffers-audio-rate-and-control-rate","text":"A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the Teensy can achieve much lower latencies than regular computers because of their lightness. Hence, the block size of your Teensy can be as small as 8!","title":"Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate"},{"location":"lectures/lecture3/#first-audio-program-on-the-teensy-crazy-sine","text":"The course repository hosts an example containing a program synthesizing a sine wave on the Teensy and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the update method and take the following shape: #define MULT_16 32767 void MyDsp::update(void) { audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { float currentSample = echo.tick(sine.tick())*0.5; currentSample = max(-1,min(1,currentSample)); int16_t val = currentSample*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } The update method is called every time a new audio buffer is needed by the system. A new audio buffer audioBlock containing AUDIO_OUTPUTS channels is first created. For every audio channel, memory is allocated and a full block of samples is computed. Individual samples resulting from computing a sine wave through an echo ( echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively) are stored in currentSample . currentSample is a floating point number whose range is {-1;1}. This is a standard in the world of digital audio, hence, a signal actually ranging between {-1;1} will correspond to the \"loudest\" sound that can be played on a given system. max(-1,min(1,currentSample)); ensures that currentSample doesn't exceed this range. AUDIO_BLOCK_SAMPLES corresponds to the block size (256 samples by default on the Teensy, but this value can potentially be adjusted). The values contained in currentSample (between -1 and 1) must be converted to 16 bits signed integers (to ensure compatibility with the rest of the Teensy audio library). For that, we just have to multiply currentSample by 2^{16}/2 Note that currentSample is multiplied by 0.5 to control the output gain of the system here (we'll see later in this class that echos tend to add energy to the system hence we must limit the gain of the output signal to prevent potential saturation). Once a full block has been computed, it is transmitted to the rest of the system using the transmit function. Once this is done, the memory that was allocated for the audio block is freed using the release function. The update method is called over and over until the Teensy is powered out.","title":"First Audio Program on the Teensy: crazy-sine"},{"location":"lectures/lecture3/#c-sine-wave-oscillator","text":"Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; }","title":"C++ Sine Wave Oscillator"},{"location":"lectures/lecture3/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture3/#looping-through-a-small-tune-making-a-music-box","text":"In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your Teensy. Hint: For that, you'll probably have to replace the myDsp.setFreq(random(50,1000)); line of of crazy-sine.ino by something else. Solution: Posted after class...","title":"Looping Through a Small Tune: Making a Music Box"},{"location":"lectures/lecture3/#basic-additive-synthesis","text":"One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Solution: Posted after class...","title":"Basic Additive Synthesis"},{"location":"lectures/lecture3/#stereo-echo","text":"Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo. Solution: Posted after class...","title":"Stereo Echo"},{"location":"lectures/lecture4/","text":"Lecture 4: Hardware Control and Audio Codec Configuration The two main goals of this lecture are: to show you how to control DSP algorithms running on your Teensy using hardware controllers (i.e., potentiometers and buttons); to give you a basic understanding of how audio codecs work and how they can be configured using the i2c protocol. Hardware Control Electronic Basics While the goal of this class is not to teach electronics nor to make projects involving complicated circuitry, some basic circuits do need to be implemented in order to control the various parameters of the DSP algorithms studied in class using hardware controllers such as buttons and potentiometers. Hence, in case you feel like your electronics skills are a bit rusty, feel free to review the following page: https://ccrma.stanford.edu/wiki/Introduction_to_Electronics_(condensed) . Teensy 4.0 Pinout The Teensy pins map can be seen in the following figure (directly taken from the PJRC website ). Most pins can be used as digital I/Os. Some pins noted \"A(N)\" can be used as analog inputs. Please, also note that 3.3v power can be retrieved from the top right corner pin and the ground from the top left pin. Make sure to never connect the 5.5v Vin pin to any other pin of the Teensy: that would probably fry it (the Cortex M7 inside the Teensy operates at 3.3v)! Teensy pinout. The Teensy audio shield uses a bunch of pins on the Teensy for i2c and i2s communication: Teensy audio shield pins. This means that these pins (besides GND and 3.3v, of course) cannot be used for something else (i.e., connecting external sensors). Bringing Power to Your Breadboard The first step in making your first circuit with the Teensy is to bring power to the breadboard included in your kit using jumper wires: Teensy connected to the breadboard. Basically connect the 3.3v pin to the red strip and the GND pin to the black strip of the breadboard. WARNING: Do not connect the 5.5v pin to the breadboard! Adding a Rotary Potentiometer to the Circuit Your kit should come with a couple of rotary potentiometers: Rotary potentiometer mounted on the breadboard. Place it on the breadboard, and connect its leftmost pin to power and its rightmost pin to the ground. Finally, connect its center pin to the A0 pin of the Teensy using a jumper wire. Please, note that we're using this pin since it is not used by the audio shield (see previous section). Testing the Potentiometer In the current configuration, the potentiometer will deliver a 3.3v current at its center pin if it is fully turned to the right side, and 0v if it is fully turned to the left side. The following Teensy program: void setup() { Serial.begin(9600); } void loop() { int sensorValue = analogRead(A0); Serial.println(sensorValue); delay(100); } displays the values measured at the A0 pin on the Teensy in the serial debugger. Values should be between 0 and 1023 (10 bits values). Make sure that the values you're getting are consistent with the position of the potentiometer. Controlling DSP Parameters With the Potentiometer Now that you know how to retrieve potentiometer values in the Teensy, plugging it to your audio DSP should be pretty straightforward. Hence, we can reuse the crazy-sine example and do: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } Note that sensorValue needs to be turned into a frequency in hertz so we just add 100 to it to get a frequency between 100 and 1123 hertz. Now take some time to have fun ;)! Using a Button Using a button with the Teensy is slightly more involving since the use of a pulldown resistor is required (alternatively, a pullup resistor could be used, of course). This is due to the fact that buttons are \"just\" circuit breakers: they don't have a dedicated output pin like potentiometers. The pulldown resistor is used to suck potential floating currents out of the output pin of the button in order to get a stable signal to be measured on the Teensy. Hence, the following circuit must be implemented: Circuit to connect a button to the Teensy. There's no need to use an analog pin on the Teensy to measure the voltage at the output of the button since we're looking at discrete values here (0 or 1). Hence, the button shall be connected to a digital pin (number 0, for example). Expanding on the previous example, we could write: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { pinMode(0, INPUT); // configuring digital pin 0 as an input AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { if (digitalRead(0)) { // button is pressed myDsp.setGain(1); } else { myDsp.setGain(0); } int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } (Assuming that a setGain method has been implemented, which is not the case in the previous example. It shouldn't be too hard though ;) ) Exercise: Looping Between Notes by Pressing a Button Expand the \"note looper\" that you implemented as part of lecture 3 so that new notes are triggered when a button is pressed (as opposed to be triggered automatically). Every time the button is pressed, a new note is produced. This means that you'll have to turn your push button into a switch using software techniques... Finally, make sure that gain is controllable using a rotary potentiometer. Audio Codec Configuration Audio Codec An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each other in practice later in this class. The Teensy Audio Shield hosts an audio codec (SGTL5000) which is connected to the Teensy using the following model: Interfacing of a microcontroller with an audio codec. You can check the Teensy audio shield connection map in the Teensy Pinout section for more details on how the audio shield is actually connected to the Teensy. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems. Quick Tour of the SGTL5000 The SGTL5000 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On the first page of the data sheet, you will find a block diagram indicating the different components of the codec: Block diagram of the SGTL5000 As you can see, the SGTL5000 hosts a stereo ADC and DAC. The codec has a stereo line input and a mono mic input (both accessible through solderable pins on the audio shield). The difference between the two is that the line input goes straight to the ADC while the mic input goes through a preamp first. The gain of the preamp can be adjusted independently from that of the line input. A similar pattern is used for the outputs and the DAC which are available as line outputs (through solderable pins on the audio shield) or amplified outputs (through the headphone jack on the audio shield). Finally, the codec also has a digital interface which is used for i2c (depicted at the bottom of the block diagram) and i2s (depicted on the left side of the block diagram) communication. Configuring an Audio Codec All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and a 16 bits value can be provided to configure them. A list of all available registers of the SGTL5000 can be seen on page 31-59 of the data sheet . Have a quick look at it! For example, register 0x0010 (which is documented on p. 36) allows us to configure the DAC left and right channel volume in dB. Hence setting that register to the 3C3C (0011110000111100 binary value) will set the volume to 0dB on both channels. If you're hex/binary is a bit rusty, you can use this tool: https://www.rapidtables.com/convert/number/hex-to-binary.html to carry out the conversion. Audio Codec Driver Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. control_sgtl5000.cpp of the Teensy Audio Library implements a driver for the SGTL5000 codec. The write method can be used to set a register and its corresponding 16-bit value. Note that the Arduino Wire library is used for that. The enable method sets a bunch of register values to provide a basic working configuration to the codec in the context of the Teensy. A bunch of methods are implemented to set high-level parameters of the codec, such as volume , micGain , lineInLevel , etc. Note that a bunch of macros are defined at the beginning for various registers (which constitutes a potential alternative to the codec datasheet, etc.). In most cases, calling: AudioControlSGTL5000 audioShield; audioShield.enable(); in the .ino file will be sufficient to get the audio codec going and send it i2s data. Since the write method of AudioControlSGTL5000 is protected, it cannot be called outside of the class. Hence, to write custom methods to configure the codec and which are not currently available in AudioControlSGTL5000 , one would have to write a custom version of control_sgtl5000.cpp .","title":" 4: Hardware Control and Audio Codec Configuration "},{"location":"lectures/lecture4/#lecture-4-hardware-control-and-audio-codec-configuration","text":"The two main goals of this lecture are: to show you how to control DSP algorithms running on your Teensy using hardware controllers (i.e., potentiometers and buttons); to give you a basic understanding of how audio codecs work and how they can be configured using the i2c protocol.","title":"Lecture 4: Hardware Control and Audio Codec Configuration"},{"location":"lectures/lecture4/#hardware-control","text":"","title":"Hardware Control"},{"location":"lectures/lecture4/#electronic-basics","text":"While the goal of this class is not to teach electronics nor to make projects involving complicated circuitry, some basic circuits do need to be implemented in order to control the various parameters of the DSP algorithms studied in class using hardware controllers such as buttons and potentiometers. Hence, in case you feel like your electronics skills are a bit rusty, feel free to review the following page: https://ccrma.stanford.edu/wiki/Introduction_to_Electronics_(condensed) .","title":"Electronic Basics"},{"location":"lectures/lecture4/#teensy-40-pinout","text":"The Teensy pins map can be seen in the following figure (directly taken from the PJRC website ). Most pins can be used as digital I/Os. Some pins noted \"A(N)\" can be used as analog inputs. Please, also note that 3.3v power can be retrieved from the top right corner pin and the ground from the top left pin. Make sure to never connect the 5.5v Vin pin to any other pin of the Teensy: that would probably fry it (the Cortex M7 inside the Teensy operates at 3.3v)! Teensy pinout. The Teensy audio shield uses a bunch of pins on the Teensy for i2c and i2s communication: Teensy audio shield pins. This means that these pins (besides GND and 3.3v, of course) cannot be used for something else (i.e., connecting external sensors).","title":"Teensy 4.0 Pinout"},{"location":"lectures/lecture4/#bringing-power-to-your-breadboard","text":"The first step in making your first circuit with the Teensy is to bring power to the breadboard included in your kit using jumper wires: Teensy connected to the breadboard. Basically connect the 3.3v pin to the red strip and the GND pin to the black strip of the breadboard. WARNING: Do not connect the 5.5v pin to the breadboard!","title":"Bringing Power to Your Breadboard"},{"location":"lectures/lecture4/#adding-a-rotary-potentiometer-to-the-circuit","text":"Your kit should come with a couple of rotary potentiometers: Rotary potentiometer mounted on the breadboard. Place it on the breadboard, and connect its leftmost pin to power and its rightmost pin to the ground. Finally, connect its center pin to the A0 pin of the Teensy using a jumper wire. Please, note that we're using this pin since it is not used by the audio shield (see previous section).","title":"Adding a Rotary Potentiometer to the Circuit"},{"location":"lectures/lecture4/#testing-the-potentiometer","text":"In the current configuration, the potentiometer will deliver a 3.3v current at its center pin if it is fully turned to the right side, and 0v if it is fully turned to the left side. The following Teensy program: void setup() { Serial.begin(9600); } void loop() { int sensorValue = analogRead(A0); Serial.println(sensorValue); delay(100); } displays the values measured at the A0 pin on the Teensy in the serial debugger. Values should be between 0 and 1023 (10 bits values). Make sure that the values you're getting are consistent with the position of the potentiometer.","title":"Testing the Potentiometer"},{"location":"lectures/lecture4/#controlling-dsp-parameters-with-the-potentiometer","text":"Now that you know how to retrieve potentiometer values in the Teensy, plugging it to your audio DSP should be pretty straightforward. Hence, we can reuse the crazy-sine example and do: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } Note that sensorValue needs to be turned into a frequency in hertz so we just add 100 to it to get a frequency between 100 and 1123 hertz. Now take some time to have fun ;)!","title":"Controlling DSP Parameters With the Potentiometer"},{"location":"lectures/lecture4/#using-a-button","text":"Using a button with the Teensy is slightly more involving since the use of a pulldown resistor is required (alternatively, a pullup resistor could be used, of course). This is due to the fact that buttons are \"just\" circuit breakers: they don't have a dedicated output pin like potentiometers. The pulldown resistor is used to suck potential floating currents out of the output pin of the button in order to get a stable signal to be measured on the Teensy. Hence, the following circuit must be implemented: Circuit to connect a button to the Teensy. There's no need to use an analog pin on the Teensy to measure the voltage at the output of the button since we're looking at discrete values here (0 or 1). Hence, the button shall be connected to a digital pin (number 0, for example). Expanding on the previous example, we could write: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { pinMode(0, INPUT); // configuring digital pin 0 as an input AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { if (digitalRead(0)) { // button is pressed myDsp.setGain(1); } else { myDsp.setGain(0); } int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } (Assuming that a setGain method has been implemented, which is not the case in the previous example. It shouldn't be too hard though ;) )","title":"Using a Button"},{"location":"lectures/lecture4/#exercise-looping-between-notes-by-pressing-a-button","text":"Expand the \"note looper\" that you implemented as part of lecture 3 so that new notes are triggered when a button is pressed (as opposed to be triggered automatically). Every time the button is pressed, a new note is produced. This means that you'll have to turn your push button into a switch using software techniques... Finally, make sure that gain is controllable using a rotary potentiometer.","title":"Exercise: Looping Between Notes by Pressing a Button"},{"location":"lectures/lecture4/#audio-codec-configuration","text":"","title":"Audio Codec Configuration"},{"location":"lectures/lecture4/#audio-codec","text":"An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each other in practice later in this class. The Teensy Audio Shield hosts an audio codec (SGTL5000) which is connected to the Teensy using the following model: Interfacing of a microcontroller with an audio codec. You can check the Teensy audio shield connection map in the Teensy Pinout section for more details on how the audio shield is actually connected to the Teensy. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems.","title":"Audio Codec"},{"location":"lectures/lecture4/#quick-tour-of-the-sgtl5000","text":"The SGTL5000 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On the first page of the data sheet, you will find a block diagram indicating the different components of the codec: Block diagram of the SGTL5000 As you can see, the SGTL5000 hosts a stereo ADC and DAC. The codec has a stereo line input and a mono mic input (both accessible through solderable pins on the audio shield). The difference between the two is that the line input goes straight to the ADC while the mic input goes through a preamp first. The gain of the preamp can be adjusted independently from that of the line input. A similar pattern is used for the outputs and the DAC which are available as line outputs (through solderable pins on the audio shield) or amplified outputs (through the headphone jack on the audio shield). Finally, the codec also has a digital interface which is used for i2c (depicted at the bottom of the block diagram) and i2s (depicted on the left side of the block diagram) communication.","title":"Quick Tour of the SGTL5000"},{"location":"lectures/lecture4/#configuring-an-audio-codec","text":"All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and a 16 bits value can be provided to configure them. A list of all available registers of the SGTL5000 can be seen on page 31-59 of the data sheet . Have a quick look at it! For example, register 0x0010 (which is documented on p. 36) allows us to configure the DAC left and right channel volume in dB. Hence setting that register to the 3C3C (0011110000111100 binary value) will set the volume to 0dB on both channels. If you're hex/binary is a bit rusty, you can use this tool: https://www.rapidtables.com/convert/number/hex-to-binary.html to carry out the conversion.","title":"Configuring an Audio Codec"},{"location":"lectures/lecture4/#audio-codec-driver","text":"Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. control_sgtl5000.cpp of the Teensy Audio Library implements a driver for the SGTL5000 codec. The write method can be used to set a register and its corresponding 16-bit value. Note that the Arduino Wire library is used for that. The enable method sets a bunch of register values to provide a basic working configuration to the codec in the context of the Teensy. A bunch of methods are implemented to set high-level parameters of the codec, such as volume , micGain , lineInLevel , etc. Note that a bunch of macros are defined at the beginning for various registers (which constitutes a potential alternative to the codec datasheet, etc.). In most cases, calling: AudioControlSGTL5000 audioShield; audioShield.enable(); in the .ino file will be sufficient to get the audio codec going and send it i2s data. Since the write method of AudioControlSGTL5000 is protected, it cannot be called outside of the class. Hence, to write custom methods to configure the codec and which are not currently available in AudioControlSGTL5000 , one would have to write a custom version of control_sgtl5000.cpp .","title":"Audio Codec Driver"},{"location":"lectures/lecture5/","text":"Lecture 5: Audio Processing Basics I This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community. White Noise White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise. Wave Shape Synthesis Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The crazy-sine example can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point. Amplitude Modulation (AM) Synthesis Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. Frequency Modulation (FM) Synthesis Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple Teensy program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had. Simple Filter: One Zero Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the Teensy demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try! Exercises LFO: Low Frequency Oscillator An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound. Solution: Shall be posted here after class... Towards the DX7 The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1. Solution: Shall be posted after class...","title":" 5: Audio Processing Basics I "},{"location":"lectures/lecture5/#lecture-5-audio-processing-basics-i","text":"This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community.","title":"Lecture 5: Audio Processing Basics I"},{"location":"lectures/lecture5/#white-noise","text":"White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise.","title":"White Noise"},{"location":"lectures/lecture5/#wave-shape-synthesis","text":"Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The crazy-sine example can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point.","title":"Wave Shape Synthesis"},{"location":"lectures/lecture5/#amplitude-modulation-am-synthesis","text":"Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value.","title":"Amplitude Modulation (AM) Synthesis"},{"location":"lectures/lecture5/#frequency-modulation-fm-synthesis","text":"Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple Teensy program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had.","title":"Frequency Modulation (FM) Synthesis"},{"location":"lectures/lecture5/#simple-filter-one-zero","text":"Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the Teensy demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try!","title":"Simple Filter: One Zero"},{"location":"lectures/lecture5/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture5/#lfo-low-frequency-oscillator","text":"An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound. Solution: Shall be posted here after class...","title":"LFO: Low Frequency Oscillator"},{"location":"lectures/lecture5/#towards-the-dx7","text":"The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1. Solution: Shall be posted after class...","title":"Towards the DX7"},{"location":"lectures/lecture6/","text":"Lecture 6: Audio Processing Basics II Harmonic Distortion: Rock On! Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the LyraT demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer. Echo An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the LyraT demonstrates the use of Echo.cpp . Comb A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the LyraT demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay. Physical Modeling: the Simple Case of the Karplus Strong Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple LyraT program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling. Exercise Smoothing In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :).","title":"Lecture 6: Audio Processing Basics II"},{"location":"lectures/lecture6/#lecture-6-audio-processing-basics-ii","text":"","title":"Lecture 6: Audio Processing Basics II"},{"location":"lectures/lecture6/#harmonic-distortion-rock-on","text":"Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the LyraT demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer.","title":"Harmonic Distortion: Rock On!"},{"location":"lectures/lecture6/#echo","text":"An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the LyraT demonstrates the use of Echo.cpp .","title":"Echo"},{"location":"lectures/lecture6/#comb","text":"A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the LyraT demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay.","title":"Comb"},{"location":"lectures/lecture6/#physical-modeling-the-simple-case-of-the-karplus-strong","text":"Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple LyraT program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling.","title":"Physical Modeling: the Simple Case of the Karplus Strong"},{"location":"lectures/lecture6/#exercise","text":"","title":"Exercise"},{"location":"lectures/lecture6/#smoothing","text":"In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :).","title":"Smoothing"},{"location":"lectures/lecture7/","text":"Lecture 7: Faust Tutorial (TBD) Faust Web site","title":"Lecture 7: Faust Tutorial (TBD)"},{"location":"lectures/lecture7/#lecture-7-faust-tutorial-tbd","text":"Faust Web site","title":"Lecture 7: Faust Tutorial (TBD)"},{"location":"lectures/lecture8/","text":"","title":"Lecture8"},{"location":"lectures/lecture9/","text":"Lecture 9: Embedded System Peripherals This course will explain in more details the principles of embedded programming, peripheral programming, an interrupt handling. Slides It is (temporarily) available through sildes here Exercice: use of timers Read the doc... Have 10 minutes looking at chapter 19 (p498) of esp32_datasheet_en.pdf : the chapter dedicated to timers. How many timers do we have ? Where are presented the registers configuring timee 0 (group 0)? More difficult: Where are defined the macros to access these register in IDF suite? Run a timer simple app go in the example/blink-timer directory of the course github, run the example and look at the code. Identify the timer ISR Identify the timer initialisation change the blink rythm is some way Print the address and the value of TIMG_T0CONFIG_REG(0) Raw Programming of timer register Here again, the API proposed by IDF is quite complex. In embedded programming, simple is beautiful, we can program all the timer register directly. Comment the line that starts the timer: ``timer_start(TIMER_GROUP_0, 0);``, does it work now ? Replace this command by a direct activation of the timer: setting the bit ``TIMG_T0_EN`` in register `` TIMG_T0CONFIG_REG(0)`` (See documentation page 501: register ``TIMGn_TxCONFIG_REG``). < -- int config_reg=read_reg(TIMG_T0CONFIG_REG(0)); config_reg |= 1<<31 ; printf(\"config_reg 0x%x\\n\", config_reg); write_reg(TIMG_T0CONFIG_REG(0),config_reg); -->","title":"Lecture 9: Embedded System Peripherals"},{"location":"lectures/lecture9/#lecture-9-embedded-system-peripherals","text":"This course will explain in more details the principles of embedded programming, peripheral programming, an interrupt handling.","title":"Lecture 9: Embedded System Peripherals"},{"location":"lectures/lecture9/#slides","text":"It is (temporarily) available through sildes here","title":"Slides"},{"location":"lectures/lecture9/#exercice-use-of-timers","text":"","title":"Exercice: use of timers"},{"location":"lectures/lecture9/#read-the-doc","text":"Have 10 minutes looking at chapter 19 (p498) of esp32_datasheet_en.pdf : the chapter dedicated to timers. How many timers do we have ? Where are presented the registers configuring timee 0 (group 0)? More difficult: Where are defined the macros to access these register in IDF suite?","title":"Read the doc..."},{"location":"lectures/lecture9/#run-a-timer-simple-app","text":"go in the example/blink-timer directory of the course github, run the example and look at the code. Identify the timer ISR Identify the timer initialisation change the blink rythm is some way Print the address and the value of TIMG_T0CONFIG_REG(0)","title":"Run a timer simple app"},{"location":"lectures/lecture9/#raw-programming-of-timer-register","text":"Here again, the API proposed by IDF is quite complex. In embedded programming, simple is beautiful, we can program all the timer register directly. Comment the line that starts the timer: ``timer_start(TIMER_GROUP_0, 0);``, does it work now ? Replace this command by a direct activation of the timer: setting the bit ``TIMG_T0_EN`` in register `` TIMG_T0CONFIG_REG(0)`` (See documentation page 501: register ``TIMGn_TxCONFIG_REG``). < -- int config_reg=read_reg(TIMG_T0CONFIG_REG(0)); config_reg |= 1<<31 ; printf(\"config_reg 0x%x\\n\", config_reg); write_reg(TIMG_T0CONFIG_REG(0),config_reg); -->","title":"Raw Programming of timer register"},{"location":"lectures/project/","text":"Personnal Project: Your Own LyraT Program The remaining course (approximately 3 TD of 2h) will be dedicated to a personal project that you will implement on LyraT, the idea it to explore further one aspect of the LyraT programming and to present it to the other students in a small d\u00e9mo during the last course. Possible Projects You are encouraged to proposed your own project, here are some idea of topics that can be studied. Note that the project will be done in a very short time , make sure to be able to present something, even if it is juste a study for a future implementation Explore Digital Synthesis or effect on the web (e.g., https://en.wikipedia.org/wiki/Category:Sound_synthesis_types , https://www.dafx.de/ , https://en.wikipedia.org/wiki/Sound_effect#techniques , https://ccrma.stanford.edu/~jos/ , etc.), and implement a particular algorithm Provide an OSC control via udp connection with Wifi Provide a BlueTooth/midi controler such as the one implemented here ( https://github.com/midibox/esp32-idf-blemidi/tree/master/components/blemidi ) Provide a USB/midi controler with an external midi/uSB device Think of a funny application using wireless communications between several LyraT Provide an HTTP server that can be used via wifi to control the LyraT program Explore (as above) digital synthesis of effect but in Faust to LyraT Rules: One or two students per project project should be presented to the class \"as it is\" on last course in less than 10mn (e.g. three slides and a demo)","title":"Personnal Project: Your Own LyraT Program"},{"location":"lectures/project/#personnal-project-your-own-lyrat-program","text":"The remaining course (approximately 3 TD of 2h) will be dedicated to a personal project that you will implement on LyraT, the idea it to explore further one aspect of the LyraT programming and to present it to the other students in a small d\u00e9mo during the last course.","title":"Personnal Project: Your Own LyraT Program"},{"location":"lectures/project/#possible-projects","text":"You are encouraged to proposed your own project, here are some idea of topics that can be studied. Note that the project will be done in a very short time , make sure to be able to present something, even if it is juste a study for a future implementation Explore Digital Synthesis or effect on the web (e.g., https://en.wikipedia.org/wiki/Category:Sound_synthesis_types , https://www.dafx.de/ , https://en.wikipedia.org/wiki/Sound_effect#techniques , https://ccrma.stanford.edu/~jos/ , etc.), and implement a particular algorithm Provide an OSC control via udp connection with Wifi Provide a BlueTooth/midi controler such as the one implemented here ( https://github.com/midibox/esp32-idf-blemidi/tree/master/components/blemidi ) Provide a USB/midi controler with an external midi/uSB device Think of a funny application using wireless communications between several LyraT Provide an HTTP server that can be used via wifi to control the LyraT program Explore (as above) digital synthesis of effect but in Faust to LyraT","title":"Possible Projects"},{"location":"lectures/project/#rules","text":"One or two students per project project should be presented to the class \"as it is\" on last course in less than 10mn (e.g. three slides and a demo)","title":"Rules:"}]}