{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Embedded Audio Signal Processing This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ) and Grame-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the forthcoming new Citi team Emeraude (Embedded Programmable Audio Systems). Instructors Romain Michon Yann Orlarey Tanguy Risset Organization and ECTS The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs","title":"Syllabus"},{"location":"#embedded-audio-signal-processing","text":"This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ) and Grame-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the forthcoming new Citi team Emeraude (Embedded Programmable Audio Systems).","title":"Embedded Audio Signal Processing"},{"location":"#instructors","text":"Romain Michon Yann Orlarey Tanguy Risset","title":"Instructors"},{"location":"#organization-and-ects","text":"The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs","title":"Organization and ECTS"},{"location":"lectures/lecture1/","text":"Lecture 1: Course Introduction and Programming Environment Setup TODO","title":" 1: Course Introduction and Programming Environment Setup "},{"location":"lectures/lecture1/#lecture-1-course-introduction-and-programming-environment-setup","text":"TODO","title":"Lecture 1: Course Introduction and Programming Environment Setup"},{"location":"lectures/lecture2/","text":"Lecture 2: Audio Signal Processing Fundamentals The goal of this TD is to provide an overview of the basics of digital audio. Analog Audio Signals Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen. This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer The Discovery of Digital Audio Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is just a variation of tension in time in a electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia ) ADC and DAC In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.). Human Hearing Range and Sampling Rate One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz. Sampling Theorem Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here . Aliasing Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening. Bit Depth, Dynamic Range and Signal-to-Noise Ratio Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here . Range of Audio Samples Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signal to this range to prevent warping and will hence result in clipping if exceeded. First Synthesized Sound on a Digital Computer While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":" 2: Audio Signal Processing Fundamentals "},{"location":"lectures/lecture2/#lecture-2-audio-signal-processing-fundamentals","text":"The goal of this TD is to provide an overview of the basics of digital audio.","title":"Lecture 2: Audio Signal Processing Fundamentals"},{"location":"lectures/lecture2/#analog-audio-signals","text":"Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen. This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer","title":"Analog Audio Signals"},{"location":"lectures/lecture2/#the-discovery-of-digital-audio","text":"Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is just a variation of tension in time in a electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia )","title":"The Discovery of Digital Audio"},{"location":"lectures/lecture2/#adc-and-dac","text":"In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.).","title":"ADC and DAC"},{"location":"lectures/lecture2/#human-hearing-range-and-sampling-rate","text":"One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz.","title":"Human Hearing Range and Sampling Rate"},{"location":"lectures/lecture2/#sampling-theorem","text":"Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here .","title":"Sampling Theorem"},{"location":"lectures/lecture2/#aliasing","text":"Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening.","title":"Aliasing"},{"location":"lectures/lecture2/#bit-depth-dynamic-range-and-signal-to-noise-ratio","text":"Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here .","title":"Bit Depth, Dynamic Range and Signal-to-Noise Ratio"},{"location":"lectures/lecture2/#range-of-audio-samples","text":"Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signal to this range to prevent warping and will hence result in clipping if exceeded.","title":"Range of Audio Samples"},{"location":"lectures/lecture2/#first-synthesized-sound-on-a-digital-computer","text":"While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":"First Synthesized Sound on a Digital Computer"},{"location":"lectures/lecture3/","text":"Lecture 3: Digital Audio Systems Architectures and Audio Callback By the end of this lecture, you should be able to produce sound with your LyraT and have a basic understanding of the software and hardware architecture of embedded audio systems. Basic Architecture of a Digital Audio System All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system. Architecture of Embedded Audio Systems Such as the LyraT Since the LyraT is and embedded system, the audio ADC and DAC are built-in in the board within a component called an audio codec. The audio codec can be seen as an audio interface providing audio inputs and outputs. It is directly connected to the ESP32 board though an i2s bus. Additional information on how this kind of system works will be provided in Lecture 4 . LyraT Overview Diagram From the ESP32 Website Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result in the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function will hence typically take the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the LyraT can achieve much lower latencies that regular computers because of their lightness. Hence, the block size of your LyraT can be as small as 16! First Audio Program on the LyraT: crazy-sine The course repository hosts an example containing a program synthesizing a sine wave on the LyraT board and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the audioTask method and take the following shape: void AudioDsp::audioTask() { // inifinite loop while (fRunning) { int16_t samples_data_out[fNumOutputs*fBufferSize]; // processing buffers for (int i = 0; i < fBufferSize; i++) { // DSP float currentSample = echo.tick(sine.tick())*0.5; // copying to output buffer samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); } // Task has to deleted itself beforee returning vTaskDelete(nullptr); } First, a while loop is implemented and is repeated every time a new buffer is needed, that's basically the \"callback\". samples_data_out is the output buffer whose size is the buffer size multiplied by the number of outputs of the system. For example, if the system has a stereo output and the buffer size is 256 samples, then the size of samples_data_out will be 512. Audio samples are coded here on 16 bits integers which is the data type accepted by the audio codec of the LyraT. Then, the audio rate for loop is implemented and samples are processed and stored in a float called currentSample . echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively. Converting Floats to int16_t Since the type of the output buffer is 16 bits signed integer, the float value of currentSample must be converted. For that, we just have to multiply currentSample by 2^{16}/2 (the range of currentSample is {-1;1}). As explained in Lecture 2 , float are used for signal processing for convenience because most algorithms are easier to deal with using decimal numbers. Interlacing Output Samples The LyraT has a stereo output (2 channels). The way parallel channels are transmitted to the audio codec is by interlacing samples in the output buffer (the same is true for the input, of course). This is carried out with the following piece of code: samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; Hence, the left channel sample is first written into the buffer, then the right channel sample and so on, etc. Here, the left channel is copied into the right channel since the DSP algorithm only has a single output. Now you should understand why samples_data_out was declared as int16_t samples_data_out[fNumOutputs*fBufferSize]; . i2s Transmission Once samples_data_out has been formatted, the buffer is transmitted to the audio codec using the i2s protocol (which will be detailed in TODO). The i2s_write uses a blocking mechanism to hold the thread (task) until a new buffer is needed. C++ Sine Wave Oscillator Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp which is used in crazy-sine is a good example of that. A sine wave table is pre-computed in the constructor of the class: sineTable = new float[SINE_TABLE_SIZE]; for(int i=0; i<SINE_TABLE_SIZE; i++){ sineTable[i] = std::sin(i*2.0*PI/SINE_TABLE_SIZE); } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The tick method of Sine.cpp is where the output samples of the sine wave are computed: float Sine::tick(){ int index = phasor*SINE_TABLE_SIZE; float currentSample = sineTable[index]*gain; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } sineTable is read at different speeds depending on the desired frequency. index is used to read the table and is computed based on phasor whose value is incremented by f/fs at every samples and then reset to zero when it's about to become greater or equal to 1. Exercises Looping Through a Small Tune In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your LyraT. Hint: For that, you'll probably have to replace the audioDsp.setFreq(rand()%(2000-50 + 1) + 50); line of of main.cpp by something else. Basic Additive Synthesis One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Stereo Echo Create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo.","title":" 3: Digital Audio Systems Architectures and Audio Callback "},{"location":"lectures/lecture3/#lecture-3-digital-audio-systems-architectures-and-audio-callback","text":"By the end of this lecture, you should be able to produce sound with your LyraT and have a basic understanding of the software and hardware architecture of embedded audio systems.","title":"Lecture 3: Digital Audio Systems Architectures and Audio Callback"},{"location":"lectures/lecture3/#basic-architecture-of-a-digital-audio-system","text":"All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system.","title":"Basic Architecture of a Digital Audio System"},{"location":"lectures/lecture3/#architecture-of-embedded-audio-systems-such-as-the-lyrat","text":"Since the LyraT is and embedded system, the audio ADC and DAC are built-in in the board within a component called an audio codec. The audio codec can be seen as an audio interface providing audio inputs and outputs. It is directly connected to the ESP32 board though an i2s bus. Additional information on how this kind of system works will be provided in Lecture 4 . LyraT Overview Diagram From the ESP32 Website","title":"Architecture of Embedded Audio Systems Such as the LyraT"},{"location":"lectures/lecture3/#concept-of-audio-blocks-buffers-audio-rate-and-control-rate","text":"A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result in the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function will hence typically take the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the LyraT can achieve much lower latencies that regular computers because of their lightness. Hence, the block size of your LyraT can be as small as 16!","title":"Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate"},{"location":"lectures/lecture3/#first-audio-program-on-the-lyrat-crazy-sine","text":"The course repository hosts an example containing a program synthesizing a sine wave on the LyraT board and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the audioTask method and take the following shape: void AudioDsp::audioTask() { // inifinite loop while (fRunning) { int16_t samples_data_out[fNumOutputs*fBufferSize]; // processing buffers for (int i = 0; i < fBufferSize; i++) { // DSP float currentSample = echo.tick(sine.tick())*0.5; // copying to output buffer samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); } // Task has to deleted itself beforee returning vTaskDelete(nullptr); } First, a while loop is implemented and is repeated every time a new buffer is needed, that's basically the \"callback\". samples_data_out is the output buffer whose size is the buffer size multiplied by the number of outputs of the system. For example, if the system has a stereo output and the buffer size is 256 samples, then the size of samples_data_out will be 512. Audio samples are coded here on 16 bits integers which is the data type accepted by the audio codec of the LyraT. Then, the audio rate for loop is implemented and samples are processed and stored in a float called currentSample . echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively.","title":"First Audio Program on the LyraT: crazy-sine"},{"location":"lectures/lecture3/#converting-floats-to-int16_t","text":"Since the type of the output buffer is 16 bits signed integer, the float value of currentSample must be converted. For that, we just have to multiply currentSample by 2^{16}/2 (the range of currentSample is {-1;1}). As explained in Lecture 2 , float are used for signal processing for convenience because most algorithms are easier to deal with using decimal numbers.","title":"Converting Floats to int16_t"},{"location":"lectures/lecture3/#interlacing-output-samples","text":"The LyraT has a stereo output (2 channels). The way parallel channels are transmitted to the audio codec is by interlacing samples in the output buffer (the same is true for the input, of course). This is carried out with the following piece of code: samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; Hence, the left channel sample is first written into the buffer, then the right channel sample and so on, etc. Here, the left channel is copied into the right channel since the DSP algorithm only has a single output. Now you should understand why samples_data_out was declared as int16_t samples_data_out[fNumOutputs*fBufferSize]; .","title":"Interlacing Output Samples"},{"location":"lectures/lecture3/#i2s-transmission","text":"Once samples_data_out has been formatted, the buffer is transmitted to the audio codec using the i2s protocol (which will be detailed in TODO). The i2s_write uses a blocking mechanism to hold the thread (task) until a new buffer is needed.","title":"i2s Transmission"},{"location":"lectures/lecture3/#c-sine-wave-oscillator","text":"Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp which is used in crazy-sine is a good example of that. A sine wave table is pre-computed in the constructor of the class: sineTable = new float[SINE_TABLE_SIZE]; for(int i=0; i<SINE_TABLE_SIZE; i++){ sineTable[i] = std::sin(i*2.0*PI/SINE_TABLE_SIZE); } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The tick method of Sine.cpp is where the output samples of the sine wave are computed: float Sine::tick(){ int index = phasor*SINE_TABLE_SIZE; float currentSample = sineTable[index]*gain; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } sineTable is read at different speeds depending on the desired frequency. index is used to read the table and is computed based on phasor whose value is incremented by f/fs at every samples and then reset to zero when it's about to become greater or equal to 1.","title":"C++ Sine Wave Oscillator"},{"location":"lectures/lecture3/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture3/#looping-through-a-small-tune","text":"In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your LyraT. Hint: For that, you'll probably have to replace the audioDsp.setFreq(rand()%(2000-50 + 1) + 50); line of of main.cpp by something else.","title":"Looping Through a Small Tune"},{"location":"lectures/lecture3/#basic-additive-synthesis","text":"One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product.","title":"Basic Additive Synthesis"},{"location":"lectures/lecture3/#stereo-echo","text":"Create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo.","title":"Stereo Echo"},{"location":"lectures/lecture4/","text":"Lecture 4:","title":"Lecture 4:"},{"location":"lectures/lecture4/#lecture-4","text":"","title":"Lecture 4:"}]}