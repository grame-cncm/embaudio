{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Embedded Audio Signal Processing This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ) and GRAME-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the forthcoming new Citi team Emeraude (Embedded Programmable Audio Systems). In this course, students will learn about: Low-level embedded systems for real-time audio signal processing Digital audio system architecture Audio codec configuration IC communication protocols Audio signal processing Audio sound synthesis and effects design The Faust programming language Instructors Romain Michon (GRAME-CNCM) Tanguy Risset (Citi Lab) Yann Orlarey Organization and ECTS The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs Course Overview Lecture 1: Course Introduction and Programming Environment Setup -- 15/09/2020 08h00-10h00 5TC-G2 Lecture 2: Audio Signal Processing Fundamentals -- 15/09/2020 10h00-12h00 5TC-G2 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 18/09/2020 14h00-16h00 5TC-G2 Lecture 4: Audio Codec Configuration -- 18/09/2020 16h00-18h00 5TC-G2 Lecture 5: Audio Processing Basics I -- 21/09/2020 08h00-10h00 5TC-G2 Lecture 6: Audio Processing Basics II -- 21/09/2020 10h00-12h00 5TC-G2 Lecture 7: RTone Talk -- 21/09/2020 14h00-16h00 5TC-G2 Lecture 8: Faust Tutorial -- 21/09/2020 16h00-18h00 5TC-G2 Lecture 9: Embedded System Peripherals -- 22/09/2020 08h00-10h00 5TC-G2 Lecture 10:Embedded OS, FreeRTOS, Embedded Linux Devices -- 22/09/2020 10h00-12h00 5TC-G2 Lecture 11: Faust on the LyraT and Advanced Control -- 22/09/2020 10h00-12h00 5TC-G2 Sessions 12-14: Mini-project 22/09/2020 16h00-18h00 5TC-G2 30/09/2020 10h00-12h00 5TC-G2 07/10/2020 10h00-12h00 5TC-G2","title":"Syllabus"},{"location":"#embedded-audio-signal-processing","text":"This course is a collaboration between Insa-Lyon (TC-Dept, Citi Lab ) and GRAME-CNCM . The objective is to foster the development of emerging embedded audio devices and to take advantage of the forthcoming new Citi team Emeraude (Embedded Programmable Audio Systems). In this course, students will learn about: Low-level embedded systems for real-time audio signal processing Digital audio system architecture Audio codec configuration IC communication protocols Audio signal processing Audio sound synthesis and effects design The Faust programming language","title":"Embedded Audio Signal Processing"},{"location":"#instructors","text":"Romain Michon (GRAME-CNCM) Tanguy Risset (Citi Lab) Yann Orlarey","title":"Instructors"},{"location":"#organization-and-ects","text":"The course will consists of 32 hours (2 ECTS) divided into 16h TD (or CM, this is equivalent) and 16h TP (two instructors): 2x2h CM 6x2h CM/TD 8x2h TP Evaluation on TPs","title":"Organization and ECTS"},{"location":"#course-overview","text":"Lecture 1: Course Introduction and Programming Environment Setup -- 15/09/2020 08h00-10h00 5TC-G2 Lecture 2: Audio Signal Processing Fundamentals -- 15/09/2020 10h00-12h00 5TC-G2 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 18/09/2020 14h00-16h00 5TC-G2 Lecture 4: Audio Codec Configuration -- 18/09/2020 16h00-18h00 5TC-G2 Lecture 5: Audio Processing Basics I -- 21/09/2020 08h00-10h00 5TC-G2 Lecture 6: Audio Processing Basics II -- 21/09/2020 10h00-12h00 5TC-G2 Lecture 7: RTone Talk -- 21/09/2020 14h00-16h00 5TC-G2 Lecture 8: Faust Tutorial -- 21/09/2020 16h00-18h00 5TC-G2 Lecture 9: Embedded System Peripherals -- 22/09/2020 08h00-10h00 5TC-G2 Lecture 10:Embedded OS, FreeRTOS, Embedded Linux Devices -- 22/09/2020 10h00-12h00 5TC-G2 Lecture 11: Faust on the LyraT and Advanced Control -- 22/09/2020 10h00-12h00 5TC-G2 Sessions 12-14: Mini-project 22/09/2020 16h00-18h00 5TC-G2 30/09/2020 10h00-12h00 5TC-G2 07/10/2020 10h00-12h00 5TC-G2","title":"Course Overview"},{"location":"lectures/lecture1/","text":"Lecture 1: Course Introduction and Programming Environment Setup This lecture is devoted to the software suite install so that everybody can follow the other lectures from Insa or from his home if it needs to be done in distant work. Course outline All Lecture (2h on a computer) are labs on the LyraT board Part 1 : Board introduction and Audio Signal Processing Basics Lecture 1 (2h): Course Introduction, programming env setup Lecture 2: Basic signal processing, Lecture 3: Audio systems architecture, audio callback Lecture 4: Hardware Audio codec configuration Lecture 5 & 6: Audio signal processing basics synthesis Lecture 7: Faust Language tutorial https://faust.grame.fr/ Part 2: Embedded Audio System Architecture Lecture 8: RTone comference on embedded systems in industry https://rtone.fr/ Lecture 9: Embedded System Peripherals Lecture 10: embedded OS, free-RTOS, embedded Audio linux devices Part 3: LyraT programming Lecture 11-14: mini project Lecture 15-16: demonstrations Introduction to AUD2020 and ESP3 ESP32 die shot, taken from zeptobars , its functional block and its pin layout, taken from esp32 datasheet The development in AUD are performed on LyraT which is developed by the espressif company. The programming environment used is esp-idf and esp-adf (esp-adf is a wrapper around esp-idf so as to enable easy audio configuration). Espressif Systems is a fabless IC design company, founded in 2008 in Shanghai, China (~200 employees in 2018). Espressif is designing and manufacturing low power wireless sensor chips. Their main product today is the ESP32 microcontroler, based on an old Xtensa processor architecture, that is sold for less the 5\u20ac and that is strongly oriented toward Internet of Things (IoT). Before that was ESP8089 and ESP8266 wifi chip ESP32 and LyraT The ESP32 is a Single 2.4 GHz Wi-Fi and Bluetooth combo chip (i.e. radio basebands and processor integrated into the same chip). The processor is and Xtensa Single or Dual-core 32-bit LX6 microprocessor that can reach 600 MIPS. It comes with a ROM of 448 KB, and a SRAM of 520KB and, as any micro-controller, with a bunch of peripherals (ADC, DAC, Touch pad, SPI, I2S, I2C, PWM, SDIO, Ethernet, UART, etc.), see its functionnal blocks . The two CPUs are named \u201cPRO_CPU\u201d and \u201cAPP_CPU\u201d (for \u201cprotocol\u201d and \u201capplication\u201d), however, for most purposes the two CPUs are interchangeable. Espressif proposes three documentations for the ESP32: : brief presentation (30 pages) of the whole ESP32 architecture, including the {\\em Memory Map} and {\\em Peripheral Pin Configurations}. esp32_technical_reference_manual_en.pdf : Complete reference guide of the chip, 600 pages, useful to understand the configuration of a particular component such as a peripheral for instance. esp32_hardware_design_guidelines_en.pdf which presents in details the interface to ESP32 chip and is mostly useful to integrate ESP32 in a new board. We use ESP32-LyraT v4.3 evaluation board, all available documentation is present on espressif web site . You will need to access to the hardware reference and the schematics of the board . LyraT simple diagram and board layout (from espressive webite) The ESP32-LyraT v3.4 is a hardware platform designed for the dual-core ESP32 audio applications, e.g., Wi-Fi or Bluetooth audio speakers, speech-based remote controllers, connected smart-home appliances with one or more audio functionality, etc. The components are quite clearly shown on Figure above, here are some precision: Output socket to connect headphones use a 3.5 mm stereo jack. The socket may be used with mobile phone headsets and is compatible with OMPT standard headsets only. It does not work with CTIA headsets. When programming (i.e. flashing) the board , the following actions must be performed: hold down the Boot button and simultaneously momentarily press the Reset button. This initiates the firmware upload mode. Then user can upload firmware through the serial port (using the flash program on the host computer). once the board is programmed (i.e. flashed) , pressing the Reset button is necessary for the new program to start. The audio chip used is the ES8388 from Everest . It is quite important because performance and properties of audio codec vary a lot from one to another. It is connected to I2C and I2S busses of the ESP32. The USB-UART port is used to have a serial communication between the ESP32 and the host computer as well as for flashing/programming the ESP32 with JTAG protocol using openocd tool. The Green 'Standby/Charging' LED indicates that the board is powered from USB. The red 'Power On' LED indicates that the board is on (there is a switch to cut it off). The 'Green' LED can be used by the user program. ESP32 developpement framework: ESP32 IDF IDF stands for IOT Development Framework, it is relatively straightforward to install it on your computer. It has been installed on TC dept machines, in /opt/esp-adf/esp-idf . In order to use it, you have to run the file export.sh located in the directory where you have installed IDF. Installing ESP32 IDF on your computer Note that IDF installation uses more than 3GB of disk space. Note also you will need to have Python3 (and not Python2.7), you can handle different version of Python on Linux using update-alternatives Follow the instruction on the espressive IDF getting started page and install IDF on your computer (The installation is quite long 10-20 minutes depending on the quality of your connection). Although it is not mandatory, it is recommended to add the IDF_PATH (which is the location where you installed ESP32-IDF) in your environment. When you have install ESP32-IDF in directory ''ESP32-IDF'', you have to source the export.sh file before building a project: source ESP32-IDF/export.sh it is not recommended to perform the source of ''export.sh'' in the profile script because it might invalidate other tool using python. Instead you can define a command for performing the source: alias get_idf='. $HOME/esp/esp-idf/export.sh' Different compilation tools used ESP32-IDF projects supports several build/compilation tools: make , cmake and idf.py (we recommend the use of idf.py tool) make , using a generic and quite complex ID_PATH/make/project.mk Makefile for all existing project. This was the original only tool used IDF, however it is being progressively replaced by the cmake compilation tools. cmake which is the recommended toolchain as make might not be supported anymore in further version. Here is an example of project compilation with cmake : mkdir build; cd build; cmake ../ make make flash make monitor idf.py is a top-level python config/build command line tool for ESP-IDF provided by espressive build. Here is an example of project compilation with idf.py : idf.py all idf.py flash idf.py monitor Getting Started on TC Machines Launching the Compilation Flow on TC Machines The idf tool chain is installed on TC machine in /opt/esp-idf/ . In order to use this tool chain, do the following commands: export IDF_TOOLS_PATH=/opt/idf_tools source /opt/esp-idf/export.sh You should get the following message (if you do not, it do not go further): [...] Go to the project directory and run: idf.py build then copy de /opt/esp-idf/examples/get-started/ to your home directory: cp /opt/esp-idf/examples/get-started/ ~/my-esp/ Go in the ``get-started/hello_world and build the project cd ~/my-esp/hello_world/ idf.py build Once the binary program built, connect the lyraT board with the usb cables, make sure that the central switch is in position 'on' . Load the program: execute the following command and do the 'flash' manipulation : push continuously on the boot button, press (shortly, but not too shortly) on the reset button, release the boot button: idf.py flash you should see the load executing.. Open the serial port on /dev/ttyUSB0 using IDF monitor facility: idf.py monitor Do not forget to reset the board once again (push on the reset button) after each flash operation otherwise your program is not started. Then you should see the board booting every 10 second. Kill the serial monitor by using the command: Ctrl-alt gr-] Flashing the LED. Go in the get-started/blink directory this program blink the LED, but the port in not configured correctly as it is an information that depends on the experimental board in which the ESP32 is used. launche the menuconfig interface, select example configuration and choose 22 for Blink_GPIO_number Known Problems Requirements are not satisfied: gdbgui>=0.13.2.0 On ubuntu, this message sometimes occurs when sourcing export.sh . We did not completely understood the problem but two solutions seemed to work: Remove explicitely the faulty dependance in ${IDF}/requirement.txt . The faulty dependance is not gdbgui but pugdbmi : comment the line mentionning pugdbmi Use the reddit solution: https://www.reddit.com/r/esp32/comments/ifgfy9/why_am_i_getting_this_gdbgui01320_error/ USB driver on MAC platforms It occurs on some MAC computers that the USB driver are not installed, you have to install it expicitely, it is explained here for instance: https://www.amstramgrame.fr/gramophone/loader/","title":" 1: Course Introduction and Programming Environment Setup "},{"location":"lectures/lecture1/#lecture-1-course-introduction-and-programming-environment-setup","text":"This lecture is devoted to the software suite install so that everybody can follow the other lectures from Insa or from his home if it needs to be done in distant work.","title":"Lecture 1: Course Introduction and Programming Environment Setup"},{"location":"lectures/lecture1/#course-outline","text":"All Lecture (2h on a computer) are labs on the LyraT board","title":"Course outline"},{"location":"lectures/lecture1/#part-1-board-introduction-and-audio-signal-processing-basics","text":"Lecture 1 (2h): Course Introduction, programming env setup Lecture 2: Basic signal processing, Lecture 3: Audio systems architecture, audio callback Lecture 4: Hardware Audio codec configuration Lecture 5 & 6: Audio signal processing basics synthesis Lecture 7: Faust Language tutorial https://faust.grame.fr/","title":"Part 1 : Board introduction and Audio Signal Processing Basics "},{"location":"lectures/lecture1/#part-2-embedded-audio-system-architecture","text":"Lecture 8: RTone comference on embedded systems in industry https://rtone.fr/ Lecture 9: Embedded System Peripherals Lecture 10: embedded OS, free-RTOS, embedded Audio linux devices","title":"Part 2: Embedded Audio System Architecture"},{"location":"lectures/lecture1/#part-3-lyrat-programming","text":"Lecture 11-14: mini project Lecture 15-16: demonstrations","title":"Part 3: LyraT programming"},{"location":"lectures/lecture1/#introduction-to-aud2020-and-esp3","text":"ESP32 die shot, taken from zeptobars , its functional block and its pin layout, taken from esp32 datasheet The development in AUD are performed on LyraT which is developed by the espressif company. The programming environment used is esp-idf and esp-adf (esp-adf is a wrapper around esp-idf so as to enable easy audio configuration). Espressif Systems is a fabless IC design company, founded in 2008 in Shanghai, China (~200 employees in 2018). Espressif is designing and manufacturing low power wireless sensor chips. Their main product today is the ESP32 microcontroler, based on an old Xtensa processor architecture, that is sold for less the 5\u20ac and that is strongly oriented toward Internet of Things (IoT). Before that was ESP8089 and ESP8266 wifi chip","title":"Introduction to AUD2020 and ESP3"},{"location":"lectures/lecture1/#esp32-and-lyrat","text":"The ESP32 is a Single 2.4 GHz Wi-Fi and Bluetooth combo chip (i.e. radio basebands and processor integrated into the same chip). The processor is and Xtensa Single or Dual-core 32-bit LX6 microprocessor that can reach 600 MIPS. It comes with a ROM of 448 KB, and a SRAM of 520KB and, as any micro-controller, with a bunch of peripherals (ADC, DAC, Touch pad, SPI, I2S, I2C, PWM, SDIO, Ethernet, UART, etc.), see its functionnal blocks . The two CPUs are named \u201cPRO_CPU\u201d and \u201cAPP_CPU\u201d (for \u201cprotocol\u201d and \u201capplication\u201d), however, for most purposes the two CPUs are interchangeable. Espressif proposes three documentations for the ESP32: : brief presentation (30 pages) of the whole ESP32 architecture, including the {\\em Memory Map} and {\\em Peripheral Pin Configurations}. esp32_technical_reference_manual_en.pdf : Complete reference guide of the chip, 600 pages, useful to understand the configuration of a particular component such as a peripheral for instance. esp32_hardware_design_guidelines_en.pdf which presents in details the interface to ESP32 chip and is mostly useful to integrate ESP32 in a new board. We use ESP32-LyraT v4.3 evaluation board, all available documentation is present on espressif web site . You will need to access to the hardware reference and the schematics of the board . LyraT simple diagram and board layout (from espressive webite) The ESP32-LyraT v3.4 is a hardware platform designed for the dual-core ESP32 audio applications, e.g., Wi-Fi or Bluetooth audio speakers, speech-based remote controllers, connected smart-home appliances with one or more audio functionality, etc. The components are quite clearly shown on Figure above, here are some precision: Output socket to connect headphones use a 3.5 mm stereo jack. The socket may be used with mobile phone headsets and is compatible with OMPT standard headsets only. It does not work with CTIA headsets. When programming (i.e. flashing) the board , the following actions must be performed: hold down the Boot button and simultaneously momentarily press the Reset button. This initiates the firmware upload mode. Then user can upload firmware through the serial port (using the flash program on the host computer). once the board is programmed (i.e. flashed) , pressing the Reset button is necessary for the new program to start. The audio chip used is the ES8388 from Everest . It is quite important because performance and properties of audio codec vary a lot from one to another. It is connected to I2C and I2S busses of the ESP32. The USB-UART port is used to have a serial communication between the ESP32 and the host computer as well as for flashing/programming the ESP32 with JTAG protocol using openocd tool. The Green 'Standby/Charging' LED indicates that the board is powered from USB. The red 'Power On' LED indicates that the board is on (there is a switch to cut it off). The 'Green' LED can be used by the user program.","title":"ESP32 and LyraT"},{"location":"lectures/lecture1/#esp32-developpement-framework-esp32-idf","text":"IDF stands for IOT Development Framework, it is relatively straightforward to install it on your computer. It has been installed on TC dept machines, in /opt/esp-adf/esp-idf . In order to use it, you have to run the file export.sh located in the directory where you have installed IDF.","title":"ESP32 developpement framework: ESP32 IDF"},{"location":"lectures/lecture1/#installing-esp32-idf-on-your-computer","text":"Note that IDF installation uses more than 3GB of disk space. Note also you will need to have Python3 (and not Python2.7), you can handle different version of Python on Linux using update-alternatives Follow the instruction on the espressive IDF getting started page and install IDF on your computer (The installation is quite long 10-20 minutes depending on the quality of your connection). Although it is not mandatory, it is recommended to add the IDF_PATH (which is the location where you installed ESP32-IDF) in your environment. When you have install ESP32-IDF in directory ''ESP32-IDF'', you have to source the export.sh file before building a project: source ESP32-IDF/export.sh it is not recommended to perform the source of ''export.sh'' in the profile script because it might invalidate other tool using python. Instead you can define a command for performing the source: alias get_idf='. $HOME/esp/esp-idf/export.sh'","title":"Installing ESP32 IDF on your computer"},{"location":"lectures/lecture1/#different-compilation-tools-used","text":"ESP32-IDF projects supports several build/compilation tools: make , cmake and idf.py (we recommend the use of idf.py tool) make , using a generic and quite complex ID_PATH/make/project.mk Makefile for all existing project. This was the original only tool used IDF, however it is being progressively replaced by the cmake compilation tools. cmake which is the recommended toolchain as make might not be supported anymore in further version. Here is an example of project compilation with cmake : mkdir build; cd build; cmake ../ make make flash make monitor idf.py is a top-level python config/build command line tool for ESP-IDF provided by espressive build. Here is an example of project compilation with idf.py : idf.py all idf.py flash idf.py monitor","title":"Different compilation tools used"},{"location":"lectures/lecture1/#getting-started-on-tc-machines","text":"","title":"Getting Started on TC Machines"},{"location":"lectures/lecture1/#launching-the-compilation-flow-on-tc-machines","text":"The idf tool chain is installed on TC machine in /opt/esp-idf/ . In order to use this tool chain, do the following commands: export IDF_TOOLS_PATH=/opt/idf_tools source /opt/esp-idf/export.sh You should get the following message (if you do not, it do not go further): [...] Go to the project directory and run: idf.py build then copy de /opt/esp-idf/examples/get-started/ to your home directory: cp /opt/esp-idf/examples/get-started/ ~/my-esp/ Go in the ``get-started/hello_world and build the project cd ~/my-esp/hello_world/ idf.py build Once the binary program built, connect the lyraT board with the usb cables, make sure that the central switch is in position 'on' . Load the program: execute the following command and do the 'flash' manipulation : push continuously on the boot button, press (shortly, but not too shortly) on the reset button, release the boot button: idf.py flash you should see the load executing.. Open the serial port on /dev/ttyUSB0 using IDF monitor facility: idf.py monitor Do not forget to reset the board once again (push on the reset button) after each flash operation otherwise your program is not started. Then you should see the board booting every 10 second. Kill the serial monitor by using the command: Ctrl-alt gr-]","title":"Launching the Compilation Flow on TC Machines"},{"location":"lectures/lecture1/#flashing-the-led","text":"Go in the get-started/blink directory this program blink the LED, but the port in not configured correctly as it is an information that depends on the experimental board in which the ESP32 is used. launche the menuconfig interface, select example configuration and choose 22 for Blink_GPIO_number","title":"Flashing the LED."},{"location":"lectures/lecture1/#known-problems","text":"","title":"Known Problems"},{"location":"lectures/lecture1/#requirements-are-not-satisfied-gdbgui01320","text":"On ubuntu, this message sometimes occurs when sourcing export.sh . We did not completely understood the problem but two solutions seemed to work: Remove explicitely the faulty dependance in ${IDF}/requirement.txt . The faulty dependance is not gdbgui but pugdbmi : comment the line mentionning pugdbmi Use the reddit solution: https://www.reddit.com/r/esp32/comments/ifgfy9/why_am_i_getting_this_gdbgui01320_error/","title":"Requirements are not satisfied: gdbgui&gt;=0.13.2.0"},{"location":"lectures/lecture1/#usb-driver-on-mac-platforms","text":"It occurs on some MAC computers that the USB driver are not installed, you have to install it expicitely, it is explained here for instance: https://www.amstramgrame.fr/gramophone/loader/","title":"USB driver on MAC platforms"},{"location":"lectures/lecture10/","text":"Lecture 10: Embedded OS, freeRTOS This course will present the important notions of embedded operating systems and explain in more details the FreeRtos operating system used on ESP32. Slides It is (temporarily) available through sildes here Exercices Creating tasks Create an IDF project that creates two FreeTtos Tasks: one is generating number each second, send via a FIFO of length 10 these numbers to the other task which is printing them on uart. The two task will have the same priority: 10. One can use the function esp_random for generating random numbers, xTaskCreate to create task and xQueueCreate , xQueueSend , xQueueReceive to communicate between the tasks. Experimenting priority and starving Try to saturate the FIFO (i.e. send faster than receiving by using vTaskDelay . Change the priority of the receiver task, set the priority to 9, does it starve the sender? Wath happens it the receiver does not receive anymore but executes a stupid loop such as this on: for(;;) { cpt++; }","title":" 10:   Embedded OS, freeRTOS "},{"location":"lectures/lecture10/#lecture-10-embedded-os-freertos","text":"This course will present the important notions of embedded operating systems and explain in more details the FreeRtos operating system used on ESP32.","title":"Lecture 10: Embedded OS, freeRTOS"},{"location":"lectures/lecture10/#slides","text":"It is (temporarily) available through sildes here","title":"Slides"},{"location":"lectures/lecture10/#exercices","text":"","title":"Exercices"},{"location":"lectures/lecture10/#creating-tasks","text":"Create an IDF project that creates two FreeTtos Tasks: one is generating number each second, send via a FIFO of length 10 these numbers to the other task which is printing them on uart. The two task will have the same priority: 10. One can use the function esp_random for generating random numbers, xTaskCreate to create task and xQueueCreate , xQueueSend , xQueueReceive to communicate between the tasks.","title":"Creating tasks"},{"location":"lectures/lecture10/#experimenting-priority-and-starving","text":"Try to saturate the FIFO (i.e. send faster than receiving by using vTaskDelay . Change the priority of the receiver task, set the priority to 9, does it starve the sender? Wath happens it the receiver does not receive anymore but executes a stupid loop such as this on: for(;;) { cpt++; }","title":"Experimenting priority and starving"},{"location":"lectures/lecture11/","text":"Lecture 11: Faust on the LyraT and Advanced Control Generating and Using a Faust C++ Object In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMininal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinial.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth LyraT example project demonstrates how FaustSynth.h can be used. First, it is included in AudioDsp.cpp and the following elements are declared in the corresponding header file : MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of AudioDsp.cpp : fDSP = new mydsp(); fDSP->init(fSampleRate); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[2]; for (int channel = 0; channel < 2; ++channel){ outputs[channel] = new float[fBufferSize]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the audioTask , we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: fDSP->compute(fBufferSize,NULL,outputs); // processing buffers for (int channel = 0; channel < 2; ++channel){ for(int i=0; i<fBufferSize; i++){ samples_data_out[i*fNumOutputs+channel] = outputs[channel][i]*MULT_S16; } } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void AudioDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void AudioDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); } Better Control on the LyraT The control of the parameters of an audio DSP object on the LyraT is relatively limited with what we have seen in this class. In this brief section, we propose a few solutions to overcome this problem. Since they are beyond the scope of this class, we wont go into too many details. MIDI MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. Implementing MIDI USB on the LyraT should be relatively straight forward but it hasn't been done yet. OSC Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 Since the LyraT has Wi-Fi built-in, implementing OSC control should also be relatively straight forward. HTTP Another common way on embedded audio systems to control parameters is to implement a simple web server. A webpage can then present the parameters to control in a user interface, etc. Hardware Control A simple way to offer a better control on the LyraT is to add physical sensors/elements to it (e.g., buttons, potentiometers, etc.). However, even though the LyraT hosts an ESP32 microcontroller, it doesn't provide any analog inputs to plug sensors to them. This can be solved by adding a sensor DAC to this device such as a MCP3008 (8 channel 10-bit ADC with SPI interface). Exercises Faust Triangle Oscillator The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the LyraT. Flanger The Faust libraries host a flanger function : pf.flanger_mono Turn your LyraT into a flanger effect processor!","title":" 11: Faust on the LyraT and Advanced Control "},{"location":"lectures/lecture11/#lecture-11-faust-on-the-lyrat-and-advanced-control","text":"","title":"Lecture 11: Faust on the LyraT and Advanced Control"},{"location":"lectures/lecture11/#generating-and-using-a-faust-c-object","text":"In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMininal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinial.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth LyraT example project demonstrates how FaustSynth.h can be used. First, it is included in AudioDsp.cpp and the following elements are declared in the corresponding header file : MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of AudioDsp.cpp : fDSP = new mydsp(); fDSP->init(fSampleRate); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[2]; for (int channel = 0; channel < 2; ++channel){ outputs[channel] = new float[fBufferSize]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the audioTask , we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: fDSP->compute(fBufferSize,NULL,outputs); // processing buffers for (int channel = 0; channel < 2; ++channel){ for(int i=0; i<fBufferSize; i++){ samples_data_out[i*fNumOutputs+channel] = outputs[channel][i]*MULT_S16; } } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void AudioDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void AudioDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); }","title":"Generating and Using a Faust C++ Object"},{"location":"lectures/lecture11/#better-control-on-the-lyrat","text":"The control of the parameters of an audio DSP object on the LyraT is relatively limited with what we have seen in this class. In this brief section, we propose a few solutions to overcome this problem. Since they are beyond the scope of this class, we wont go into too many details.","title":"Better Control on the LyraT"},{"location":"lectures/lecture11/#midi","text":"MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. Implementing MIDI USB on the LyraT should be relatively straight forward but it hasn't been done yet.","title":"MIDI"},{"location":"lectures/lecture11/#osc","text":"Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 Since the LyraT has Wi-Fi built-in, implementing OSC control should also be relatively straight forward.","title":"OSC"},{"location":"lectures/lecture11/#http","text":"Another common way on embedded audio systems to control parameters is to implement a simple web server. A webpage can then present the parameters to control in a user interface, etc.","title":"HTTP"},{"location":"lectures/lecture11/#hardware-control","text":"A simple way to offer a better control on the LyraT is to add physical sensors/elements to it (e.g., buttons, potentiometers, etc.). However, even though the LyraT hosts an ESP32 microcontroller, it doesn't provide any analog inputs to plug sensors to them. This can be solved by adding a sensor DAC to this device such as a MCP3008 (8 channel 10-bit ADC with SPI interface).","title":"Hardware Control"},{"location":"lectures/lecture11/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture11/#faust-triangle-oscillator","text":"The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the LyraT.","title":"Faust Triangle Oscillator"},{"location":"lectures/lecture11/#flanger","text":"The Faust libraries host a flanger function : pf.flanger_mono Turn your LyraT into a flanger effect processor!","title":"Flanger"},{"location":"lectures/lecture2/","text":"Lecture 2: Audio Signal Processing Fundamentals The goal of this lecture is to provide an overview of the basics of digital audio. Analog Audio Signals Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer The Discovery of Digital Audio Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is just a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia ) ADC and DAC In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.). Human Hearing Range and Sampling Rate One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz. Sampling Theorem Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here . Aliasing Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening. Bit Depth, Dynamic Range and Signal-to-Noise Ratio Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here . Range of Audio Samples Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signal to this range to prevent warping and will hence result in clipping if exceeded. First Synthesized Sound on a Digital Computer While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":" 2: Audio Signal Processing Fundamentals "},{"location":"lectures/lecture2/#lecture-2-audio-signal-processing-fundamentals","text":"The goal of this lecture is to provide an overview of the basics of digital audio.","title":"Lecture 2: Audio Signal Processing Fundamentals"},{"location":"lectures/lecture2/#analog-audio-signals","text":"Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer","title":"Analog Audio Signals"},{"location":"lectures/lecture2/#the-discovery-of-digital-audio","text":"Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is just a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia )","title":"The Discovery of Digital Audio"},{"location":"lectures/lecture2/#adc-and-dac","text":"In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.).","title":"ADC and DAC"},{"location":"lectures/lecture2/#human-hearing-range-and-sampling-rate","text":"One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz.","title":"Human Hearing Range and Sampling Rate"},{"location":"lectures/lecture2/#sampling-theorem","text":"Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here .","title":"Sampling Theorem"},{"location":"lectures/lecture2/#aliasing","text":"Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening.","title":"Aliasing"},{"location":"lectures/lecture2/#bit-depth-dynamic-range-and-signal-to-noise-ratio","text":"Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here .","title":"Bit Depth, Dynamic Range and Signal-to-Noise Ratio"},{"location":"lectures/lecture2/#range-of-audio-samples","text":"Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signal to this range to prevent warping and will hence result in clipping if exceeded.","title":"Range of Audio Samples"},{"location":"lectures/lecture2/#first-synthesized-sound-on-a-digital-computer","text":"While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":"First Synthesized Sound on a Digital Computer"},{"location":"lectures/lecture3/","text":"Lecture 3: Digital Audio Systems Architectures and Audio Callback By the end of this lecture, you should be able to produce sound with your LyraT and have a basic understanding of the software and hardware architecture of embedded audio systems. Basic Architecture of a Digital Audio System All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system. Architecture of Embedded Audio Systems Such as the LyraT Since the LyraT is and embedded system, the audio ADC and DAC are built-in in the board within a component called an audio codec. The audio codec can be seen as an audio interface providing audio inputs and outputs. It is directly connected to the ESP32 board though an i2s bus. Additional information on how this kind of system works will be provided in Lecture 4 . LyraT Overview Diagram From the ESP32 Website Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the LyraT can achieve much lower latencies that regular computers because of their lightness. Hence, the block size of your LyraT can be as small as 16! First Audio Program on the LyraT: crazy-sine The course repository hosts an example containing a program synthesizing a sine wave on the LyraT board and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the audioTask method and take the following shape: void AudioDsp::audioTask() { // inifinite loop while (fRunning) { int16_t samples_data_out[fNumOutputs*fBufferSize]; // processing buffers for (int i = 0; i < fBufferSize; i++) { // DSP float currentSample = echo.tick(sine.tick())*0.5; // copying to output buffer samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); } // Task has to deleted itself beforee returning vTaskDelete(nullptr); } First, a while loop is implemented and is repeated every time a new buffer is needed, that's basically the \"callback\". samples_data_out is the output buffer whose size is the buffer size multiplied by the number of outputs of the system. For example, if the system has a stereo output and the buffer size is 256 samples, then the size of samples_data_out will be 512. Audio samples are coded here on 16 bits integers which is the data type accepted by the audio codec of the LyraT. Then, the audio rate for loop is implemented and samples are processed and stored in a float called currentSample . echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively. Note that currentSample is multiplied by 0.5 to control the output gain of the system here. Converting Floats to int16_t Since the type of the output buffer is 16 bits signed integer, the float value of currentSample must be converted. For that, we just have to multiply currentSample by 2^{16}/2 (the range of currentSample is {-1;1}). As explained in Lecture 2 , float are used for signal processing for convenience because most algorithms are easier to deal with using decimal numbers. Interlacing Output Samples The LyraT has a stereo output (2 channels). The way parallel channels are transmitted to the audio codec is by interlacing samples in the output buffer (the same is true for the input, of course). This is carried out with the following piece of code: samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; Hence, the left channel sample is first written into the buffer, then the right channel sample and so on, etc. Here, the left channel is copied into the right channel since the DSP algorithm only has a single output. Now you should understand why samples_data_out was declared as int16_t samples_data_out[fNumOutputs*fBufferSize]; ;). i2s Transmission Once samples_data_out has been formatted, the buffer is transmitted to the audio codec using the i2s protocol. The i2s_write uses a blocking mechanism to hold the thread (task) until a new buffer is needed. C++ Sine Wave Oscillator Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; } Exercises Looping Through a Small Tune In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your LyraT. Hint: For that, you'll probably have to replace the audioDsp.setFreq(rand()%(2000-50 + 1) + 50); line of of main.cpp by something else. Basic Additive Synthesis One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Stereo Echo Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo.","title":" 3: Digital Audio Systems Architectures and Audio Callback "},{"location":"lectures/lecture3/#lecture-3-digital-audio-systems-architectures-and-audio-callback","text":"By the end of this lecture, you should be able to produce sound with your LyraT and have a basic understanding of the software and hardware architecture of embedded audio systems.","title":"Lecture 3: Digital Audio Systems Architectures and Audio Callback"},{"location":"lectures/lecture3/#basic-architecture-of-a-digital-audio-system","text":"All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system.","title":"Basic Architecture of a Digital Audio System"},{"location":"lectures/lecture3/#architecture-of-embedded-audio-systems-such-as-the-lyrat","text":"Since the LyraT is and embedded system, the audio ADC and DAC are built-in in the board within a component called an audio codec. The audio codec can be seen as an audio interface providing audio inputs and outputs. It is directly connected to the ESP32 board though an i2s bus. Additional information on how this kind of system works will be provided in Lecture 4 . LyraT Overview Diagram From the ESP32 Website","title":"Architecture of Embedded Audio Systems Such as the LyraT"},{"location":"lectures/lecture3/#concept-of-audio-blocks-buffers-audio-rate-and-control-rate","text":"A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the LyraT can achieve much lower latencies that regular computers because of their lightness. Hence, the block size of your LyraT can be as small as 16!","title":"Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate"},{"location":"lectures/lecture3/#first-audio-program-on-the-lyrat-crazy-sine","text":"The course repository hosts an example containing a program synthesizing a sine wave on the LyraT board and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the audioTask method and take the following shape: void AudioDsp::audioTask() { // inifinite loop while (fRunning) { int16_t samples_data_out[fNumOutputs*fBufferSize]; // processing buffers for (int i = 0; i < fBufferSize; i++) { // DSP float currentSample = echo.tick(sine.tick())*0.5; // copying to output buffer samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; } // transmitting output buffer size_t bytes_written = 0; i2s_write((i2s_port_t)0, &samples_data_out, fNumOutputs*sizeof(int16_t)*fBufferSize, &bytes_written, portMAX_DELAY); } // Task has to deleted itself beforee returning vTaskDelete(nullptr); } First, a while loop is implemented and is repeated every time a new buffer is needed, that's basically the \"callback\". samples_data_out is the output buffer whose size is the buffer size multiplied by the number of outputs of the system. For example, if the system has a stereo output and the buffer size is 256 samples, then the size of samples_data_out will be 512. Audio samples are coded here on 16 bits integers which is the data type accepted by the audio codec of the LyraT. Then, the audio rate for loop is implemented and samples are processed and stored in a float called currentSample . echo and sine are defined in the lib folder and implement an echo and a sine wave oscillator, respectively. Note that currentSample is multiplied by 0.5 to control the output gain of the system here.","title":"First Audio Program on the LyraT: crazy-sine"},{"location":"lectures/lecture3/#converting-floats-to-int16_t","text":"Since the type of the output buffer is 16 bits signed integer, the float value of currentSample must be converted. For that, we just have to multiply currentSample by 2^{16}/2 (the range of currentSample is {-1;1}). As explained in Lecture 2 , float are used for signal processing for convenience because most algorithms are easier to deal with using decimal numbers.","title":"Converting Floats to int16_t"},{"location":"lectures/lecture3/#interlacing-output-samples","text":"The LyraT has a stereo output (2 channels). The way parallel channels are transmitted to the audio codec is by interlacing samples in the output buffer (the same is true for the input, of course). This is carried out with the following piece of code: samples_data_out[i*fNumOutputs] = currentSample*MULT_S16; samples_data_out[i*fNumOutputs+1] = samples_data_out[i*fNumOutputs]; Hence, the left channel sample is first written into the buffer, then the right channel sample and so on, etc. Here, the left channel is copied into the right channel since the DSP algorithm only has a single output. Now you should understand why samples_data_out was declared as int16_t samples_data_out[fNumOutputs*fBufferSize]; ;).","title":"Interlacing Output Samples"},{"location":"lectures/lecture3/#i2s-transmission","text":"Once samples_data_out has been formatted, the buffer is transmitted to the audio codec using the i2s protocol. The i2s_write uses a blocking mechanism to hold the thread (task) until a new buffer is needed.","title":"i2s Transmission"},{"location":"lectures/lecture3/#c-sine-wave-oscillator","text":"Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; }","title":"C++ Sine Wave Oscillator"},{"location":"lectures/lecture3/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture3/#looping-through-a-small-tune","text":"In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your LyraT. Hint: For that, you'll probably have to replace the audioDsp.setFreq(rand()%(2000-50 + 1) + 50); line of of main.cpp by something else.","title":"Looping Through a Small Tune"},{"location":"lectures/lecture3/#basic-additive-synthesis","text":"One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product.","title":"Basic Additive Synthesis"},{"location":"lectures/lecture3/#stereo-echo","text":"Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo.","title":"Stereo Echo"},{"location":"lectures/lecture4/","text":"Lecture 4: Audio Codec Configuration The goal of this lecture is to get a basic understanding of how audio codecs work and how they can be configured using the i2c protocol. By the end of it, you should be able to write a simple audio codec driver. Audio Codec An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each others in practice later in this class. The LyraT hosts an audio codec (an Everest ES8388) which is connected to the ESP32 using the following model: Interfacing of a microcontroller with an audio codec. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems. Quick Tour of the ES8388 The Everest ES8388 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On page 4 of the data sheet, you will find a (not so clear) block diagram indicating the different components of the codec: Block diagram of the ES8388 and on the following page a pin map: Pin map of the ES8388 As you can see, the ES8388 hosts a stereo ADC and DAC. The codec has 4 inputs (LIN1, LIN2, RIN1, and RIN2) which can be routed to the left or right channel of the ADC. A similar pattern is used for the outputs and the DAC. Various mixers ( mux on the diagram) can be used to route the signals. The codec also hosts 2 preamps which are required to adjust the gain of potential microphones. For instance, the LyraT board hosts 2 electret mics which are directly connected to the LIN1 and LIN2 inputs of the codec (as shown in the Analog System section of the LyraT schematics ). The output level of electret mic is extremely low compared to the \"line in level\" expected by the inputs of the ADC. The role of the preamps is to raise the level of the mics so that it matches that of the ADC inputs (we call this impedance matching). Finally, the codec also has a digital interface (at the bottom of the block diagram) which is used for i2c and i2s communication. Configuring an Audio Codec All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and an 8 bits value can be provided to configure them. A list of all available registers of the ES8388 can be seen on page 11-13 of the data sheet . Have a quick look at it! For example, register 9 (which is documented on page 16 and 17) allows us to configure the level of the left and right channel preamps. If register 9 is set to 0010 0010 then the level of both the left and right preamps will be set to +6dB. Audio Codec Driver Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. ES8388.cpp implements a simple driver for the ES8388 codec. The initI2C method configures i2c by providing the correct pin numbers, etc. The writeReg method sets a register and its corresponding 8-bit value. Finally, the init method configures the audio codec by setting various registers in a mode that will work for most applications that we will study in this class. Note that ES8388.h contains a set pre-formatted macros for each register. In most cases, calling: ES8388 es8388; es8388.init(); in main.cpp will be sufficient to get the audio codec going and send it i2s data. Have a look at the crazy-sine example to get an idea of how this works. Once es8388.init(); has been called, es8388.writeReg can be called to override existing register values or set new ones. For example, the bit depth of the DAC could be set to 24 bits (instead of 16 now) by running: writeReg(ES8388_DACCONTROL1,0); which corresponds to 0000 0000 in binary format (see the documentation of register 23 on page 22 of the codec data sheet). The value of ES8388_DACCONTROL1 in init is: writeReg(ES8388_DACCONTROL1,24); and corresponds to 16 bits. 24 in binary format is: 0001 1000. If you don't know how to convert decimal numbers to binary and vice versa, have a look at this page . Exercises Deactivating Power on the ADC In the current configuration, both the ADC and the DAC are turned on. However, the crazy-sine example only uses the DAC. Hence, it might be smart to turn off the ADC to save some power. Turn off of the ADC by setting the right register. You'll have to look at the ES8388 data sheet for that, of course. Changing the Gain of the Mic Preamp Set the gain of the left microphone preamp to 21 dB and to 12 dB for the right microphone preamp. Use Line In Instead of Mic In the current configuration, the two microphones of the LyraT are used as the inputs to the codec. In some cases, we might want to use the line in instead (e.g., to implement a guitar pedal effect). Switch the input of the ADC to the line input. Hint: You might have to look at the LyraT schematics for that. Changing the Gain of the Output Set the output level of the right channel to -96 dB. Hint: There might be multiple answers to this question. Pick up the one you prefer. Changing the Sampling Rate That one is slightly harder ;), change the sampling rate of the crazy-sine example to 32kHz.","title":" 4: Audio Codec Configuration "},{"location":"lectures/lecture4/#lecture-4-audio-codec-configuration","text":"The goal of this lecture is to get a basic understanding of how audio codecs work and how they can be configured using the i2c protocol. By the end of it, you should be able to write a simple audio codec driver.","title":"Lecture 4: Audio Codec Configuration"},{"location":"lectures/lecture4/#audio-codec","text":"An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each others in practice later in this class. The LyraT hosts an audio codec (an Everest ES8388) which is connected to the ESP32 using the following model: Interfacing of a microcontroller with an audio codec. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems.","title":"Audio Codec"},{"location":"lectures/lecture4/#quick-tour-of-the-es8388","text":"The Everest ES8388 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On page 4 of the data sheet, you will find a (not so clear) block diagram indicating the different components of the codec: Block diagram of the ES8388 and on the following page a pin map: Pin map of the ES8388 As you can see, the ES8388 hosts a stereo ADC and DAC. The codec has 4 inputs (LIN1, LIN2, RIN1, and RIN2) which can be routed to the left or right channel of the ADC. A similar pattern is used for the outputs and the DAC. Various mixers ( mux on the diagram) can be used to route the signals. The codec also hosts 2 preamps which are required to adjust the gain of potential microphones. For instance, the LyraT board hosts 2 electret mics which are directly connected to the LIN1 and LIN2 inputs of the codec (as shown in the Analog System section of the LyraT schematics ). The output level of electret mic is extremely low compared to the \"line in level\" expected by the inputs of the ADC. The role of the preamps is to raise the level of the mics so that it matches that of the ADC inputs (we call this impedance matching). Finally, the codec also has a digital interface (at the bottom of the block diagram) which is used for i2c and i2s communication.","title":"Quick Tour of the ES8388"},{"location":"lectures/lecture4/#configuring-an-audio-codec","text":"All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and an 8 bits value can be provided to configure them. A list of all available registers of the ES8388 can be seen on page 11-13 of the data sheet . Have a quick look at it! For example, register 9 (which is documented on page 16 and 17) allows us to configure the level of the left and right channel preamps. If register 9 is set to 0010 0010 then the level of both the left and right preamps will be set to +6dB.","title":"Configuring an Audio Codec"},{"location":"lectures/lecture4/#audio-codec-driver","text":"Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. ES8388.cpp implements a simple driver for the ES8388 codec. The initI2C method configures i2c by providing the correct pin numbers, etc. The writeReg method sets a register and its corresponding 8-bit value. Finally, the init method configures the audio codec by setting various registers in a mode that will work for most applications that we will study in this class. Note that ES8388.h contains a set pre-formatted macros for each register. In most cases, calling: ES8388 es8388; es8388.init(); in main.cpp will be sufficient to get the audio codec going and send it i2s data. Have a look at the crazy-sine example to get an idea of how this works. Once es8388.init(); has been called, es8388.writeReg can be called to override existing register values or set new ones. For example, the bit depth of the DAC could be set to 24 bits (instead of 16 now) by running: writeReg(ES8388_DACCONTROL1,0); which corresponds to 0000 0000 in binary format (see the documentation of register 23 on page 22 of the codec data sheet). The value of ES8388_DACCONTROL1 in init is: writeReg(ES8388_DACCONTROL1,24); and corresponds to 16 bits. 24 in binary format is: 0001 1000. If you don't know how to convert decimal numbers to binary and vice versa, have a look at this page .","title":"Audio Codec Driver"},{"location":"lectures/lecture4/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture4/#deactivating-power-on-the-adc","text":"In the current configuration, both the ADC and the DAC are turned on. However, the crazy-sine example only uses the DAC. Hence, it might be smart to turn off the ADC to save some power. Turn off of the ADC by setting the right register. You'll have to look at the ES8388 data sheet for that, of course.","title":"Deactivating Power on the ADC"},{"location":"lectures/lecture4/#changing-the-gain-of-the-mic-preamp","text":"Set the gain of the left microphone preamp to 21 dB and to 12 dB for the right microphone preamp.","title":"Changing the Gain of the Mic Preamp"},{"location":"lectures/lecture4/#use-line-in-instead-of-mic","text":"In the current configuration, the two microphones of the LyraT are used as the inputs to the codec. In some cases, we might want to use the line in instead (e.g., to implement a guitar pedal effect). Switch the input of the ADC to the line input. Hint: You might have to look at the LyraT schematics for that.","title":"Use Line In Instead of Mic"},{"location":"lectures/lecture4/#changing-the-gain-of-the-output","text":"Set the output level of the right channel to -96 dB. Hint: There might be multiple answers to this question. Pick up the one you prefer.","title":"Changing the Gain of the Output"},{"location":"lectures/lecture4/#changing-the-sampling-rate","text":"That one is slightly harder ;), change the sampling rate of the crazy-sine example to 32kHz.","title":"Changing the Sampling Rate"},{"location":"lectures/lecture5/","text":"Lecture 5: Audio Processing Basics I This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community. White Noise White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise. Wave Shape Synthesis Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The sine-control and crazy-sine examples can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point. Amplitude Modulation (AM) Synthesis Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. Frequency Modulation (FM) Synthesis Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple LyraT program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had. Simple Filter: One Zero Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the LyraT demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try! Exercises LFO: Low Frequency Oscillator An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound. Towards the DX7 The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1.","title":" 5: Audio Processing Basics I "},{"location":"lectures/lecture5/#lecture-5-audio-processing-basics-i","text":"This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community.","title":"Lecture 5: Audio Processing Basics I"},{"location":"lectures/lecture5/#white-noise","text":"White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise.","title":"White Noise"},{"location":"lectures/lecture5/#wave-shape-synthesis","text":"Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The sine-control and crazy-sine examples can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point.","title":"Wave Shape Synthesis"},{"location":"lectures/lecture5/#amplitude-modulation-am-synthesis","text":"Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value.","title":"Amplitude Modulation (AM) Synthesis"},{"location":"lectures/lecture5/#frequency-modulation-fm-synthesis","text":"Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple LyraT program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had.","title":"Frequency Modulation (FM) Synthesis"},{"location":"lectures/lecture5/#simple-filter-one-zero","text":"Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the LyraT demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try!","title":"Simple Filter: One Zero"},{"location":"lectures/lecture5/#exercises","text":"","title":"Exercises"},{"location":"lectures/lecture5/#lfo-low-frequency-oscillator","text":"An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound.","title":"LFO: Low Frequency Oscillator"},{"location":"lectures/lecture5/#towards-the-dx7","text":"The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1.","title":"Towards the DX7"},{"location":"lectures/lecture6/","text":"Lecture 6: Audio Processing Basics II Harmonic Distortion: Rock On! Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the LyraT demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer. Echo An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the LyraT demonstrates the use of Echo.cpp . Comb A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the LyraT demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay. Physical Modeling: the Simple Case of the Karplus Strong Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple LyraT program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling. Exercise Smoothing In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :).","title":" 6: Audio Processing Basics II "},{"location":"lectures/lecture6/#lecture-6-audio-processing-basics-ii","text":"","title":"Lecture 6: Audio Processing Basics II"},{"location":"lectures/lecture6/#harmonic-distortion-rock-on","text":"Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the LyraT demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer.","title":"Harmonic Distortion: Rock On!"},{"location":"lectures/lecture6/#echo","text":"An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the LyraT demonstrates the use of Echo.cpp .","title":"Echo"},{"location":"lectures/lecture6/#comb","text":"A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the LyraT demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay.","title":"Comb"},{"location":"lectures/lecture6/#physical-modeling-the-simple-case-of-the-karplus-strong","text":"Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple LyraT program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling.","title":"Physical Modeling: the Simple Case of the Karplus Strong"},{"location":"lectures/lecture6/#exercise","text":"","title":"Exercise"},{"location":"lectures/lecture6/#smoothing","text":"In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :).","title":"Smoothing"},{"location":"lectures/lecture7/","text":"Lecture 7: Faust Tutorial (TBD) Faust Web site","title":" 7:  Faust Tutorial "},{"location":"lectures/lecture7/#lecture-7-faust-tutorial-tbd","text":"Faust Web site","title":"Lecture 7: Faust Tutorial (TBD)"},{"location":"lectures/lecture8/","text":"","title":" 8:   Rtone conference "},{"location":"lectures/lecture9/","text":"Lecture 9: Embedded System Peripherals This course will explain in more details the principles of embedded programming, peripheral programming, an interrupt handling. Slides It is (temporarily) available through sildes here Exercice: use of timers Read the doc... Have 10 minutes looking at chapter 19 (p498) of esp32_datasheet_en.pdf : the chapter dedicated to timers. How many timers do we have ? Where are presented the registers configuring timee 0 (group 0)? More difficult: Where are defined the macros to access these register in IDF suite? Run a timer simple app go in the example/blink-timer directory of the course github, run the example and look at the code. Identify the timer ISR Identify the timer initialisation change the blink rythm is some way Print the address and the value of TIMG_T0CONFIG_REG(0) Raw Programming of timer register Here again, the API proposed by IDF is quite complex. In embedded programming, simple is beautiful, we can program all the timer register directly. Comment the line that starts the timer: timer_start(TIMER_GROUP_0, 0); , does it work now ? Replace this command by a direct activation of the timer: setting the bit TIMG_T0_EN in register TIMG_T0CONFIG_REG(0) (See documentation page 501: register TIMGn_TxCONFIG_REG ).","title":" 9:  Embedded System Peripherals "},{"location":"lectures/lecture9/#lecture-9-embedded-system-peripherals","text":"This course will explain in more details the principles of embedded programming, peripheral programming, an interrupt handling.","title":"Lecture 9: Embedded System Peripherals"},{"location":"lectures/lecture9/#slides","text":"It is (temporarily) available through sildes here","title":"Slides"},{"location":"lectures/lecture9/#exercice-use-of-timers","text":"","title":"Exercice: use of timers"},{"location":"lectures/lecture9/#read-the-doc","text":"Have 10 minutes looking at chapter 19 (p498) of esp32_datasheet_en.pdf : the chapter dedicated to timers. How many timers do we have ? Where are presented the registers configuring timee 0 (group 0)? More difficult: Where are defined the macros to access these register in IDF suite?","title":"Read the doc..."},{"location":"lectures/lecture9/#run-a-timer-simple-app","text":"go in the example/blink-timer directory of the course github, run the example and look at the code. Identify the timer ISR Identify the timer initialisation change the blink rythm is some way Print the address and the value of TIMG_T0CONFIG_REG(0)","title":"Run a timer simple app"},{"location":"lectures/lecture9/#raw-programming-of-timer-register","text":"Here again, the API proposed by IDF is quite complex. In embedded programming, simple is beautiful, we can program all the timer register directly. Comment the line that starts the timer: timer_start(TIMER_GROUP_0, 0); , does it work now ? Replace this command by a direct activation of the timer: setting the bit TIMG_T0_EN in register TIMG_T0CONFIG_REG(0) (See documentation page 501: register TIMGn_TxCONFIG_REG ).","title":"Raw Programming of timer register"}]}